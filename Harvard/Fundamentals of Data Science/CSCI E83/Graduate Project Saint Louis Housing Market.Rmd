---
output: 
  pdf_document: #html_document #word_document
          fig_width: 10
          fig_height: 9
          highlight: "tango" #"zenburn" #"tango" #"pygments" #"espresso" #"haddock"
          df_print: "kable" #"tibble"
          # geometry: "margin=0.75in"
title: "Analysis of the Greater Saint Louis Housing Market"
thanks: "CSCI-E83 Fundamentals of Data Science, Graduate Project"
author: 
- "Paul M. Washburn"
- "CSCI E-83 Fundamentals of Data Science"
affiliation: "Harvard University"
date: "Submitted on May 12, 2017"
abstract: "The Saint Louis, Missouri community is characterized by its schools, restaurants and municipalities. With [ninety distinct municipalities](https://en.wikipedia.org/wiki/Municipalities_of_St._Louis_County,_Missouri), understanding the dominant factors at play in the housing market is of considerable economic interest. This inquiry utilizes several sources of data in an attempt to gain understanding of the mechanics influencing listing prices and price per square foot in the Saint Louis market, and translate this understanding into predictive algorithms capable of identifying opportunities in the market outside of the limited sample considered. Regressing listing price on various features yielded a strong multiple regression model for predicting the price of a given property, given certain attributes. To investigate PricePerSqFt a conditional inference tree was employed, yielding interesting insights that can be elaborated upon in future research."
always_allow_html: yes
keywords: "Data Science, Saint Louis MO, STL, St Louis, Paul Washburn, Paul Matthew Washburn, Missouri, Real Estate, Residential Real Estate, House Prices Missouri, House Prices, Machine Learning, Linear Regression, ggplot2, R, Python, R Markdown, yaml, Harvard, Harvard Extension School, Home Listings MO"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy=TRUE, error=FALSE, message=FALSE, highlight=TRUE)
library(ggplot2)
library(plotly)
library(scales)
library(ggmap)
library(googleVis)
```

# Context & Objectives

This inquiry attempts to gain insight into the market pricing mechanics at work in the Saint Louis, Missouri housing market. The goal if this inquiry is to establish a base understanding of current market dynamics in order to guide future research into the topic, with the ultimate objective of deriving predictive models for both Price and PricePerSqFt. Data on residential properties that were for sale during the Spring of 2017 was gathered & combined with data on various features that describe the surrounding area. These models may then be  used in production to identify opportunities and to help realtors advise their clients using scientifically valid, data-backed insights. This is the first investigation into the matter, and will be followed by several subsequent analyses to elaborate on the findings.


# Data Acquisition

### Home Listings
The home listing data used in this investigation was acquired, in its raw form, by simply searching for 15 pre-defined municipalities. Two of the most trusted real estate websites were used to search for listings in these cities; [realtor.com](http://www.realtor.com/realestateandhomes-search/Saint-Louis_MO?pgsz=50) and [zillow.com](https://www.zillow.com/homes/for_sale/saint-louis-mo_rb/?fromHomePage=true&shouldFireSellPageImplicitClaimGA=false&fromHomePageTab=buy). To ensure compliance with these companys' data access policies, the data was not scraped directly from their websites, but rather:

* Each municipality was searched for one-by-one in attempt at deep municipal exploration
* Each webpage in the search was saved via "View Source/CTRL+U" as an HTML document
* The HTML documents saved locally were then processed using BeautifulSoup

After manually obtaining over 50 HTML webpages from both sites, both sources were manually inspected for consistency as well as to identify the [HTML tags](http://www.pcmag.com/encyclopedia/term/44497/html-tag) that the website developers had used to render information on listing addresses and postal codes. The reason that only address and postal code were obtained using this method was to ensure data integrity. The raw webpages were scraped using [Python's BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) module, a tool which lowers considerably the barrier for engineers to scrape semantics from websites. The Python code below outlines the process of using BeautifulSoup to acquire data from the raw HTML files.

```{python eval=FALSE}
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import bs4

## Files saved as "realtor_N.html"
txt = 'C:/Users/paulm/Desktop/Harvard/Realty Data/realtor_'
URL1,URL2,URL4,URL4 = [txt+str(i)+'.html' for i in range(1, 5)]
urls = [URL1,URL2,URL3,URL4]

## Declare empty DF to gather data
LISTING_DF = pd.DataFrame()

## Enumerate data to acquire and loop through files
attributes = ['listing-street-address','listing-city','listing-region','listing-postal']
namez = ['Address','City','State','Zip']
for i, l in enumerate(urls):
    soup = BeautifulSoup(open(l))
    ROWS = soup.find_all('span',{'class':attributes})
    new_listing = {}
    for j, row in enumerate(ROWS):
        new_listing[j%4+1] = row.text
        df = pd.DataFrame.from_dict(new_listing, 'index')
        if len(df) == 4:
            df = df.transpose()
            df.rename(columns={1:'Address',2:'City',3:'State',4:'Zip'}, inplace=True)
            ix = df.Address.astype(str)#+'_'+df.City.astype(str)+'_'+df.Zip.astype(str)
            df.set_index(ix, inplace=True)
            LISTING_DF = LISTING_DF.append(df)
        else:
            del df
    LISTING_DF.drop_duplicates(subset='Address', inplace=True)
    LISTING_DF.dropna(inplace=True)
```

As address/zip code pairs were obtained, [Zillow's API](https://www.zillow.com/howto/api/APIOverview.htm) was employed in an iterative process until a sufficient number of quality observations were obtained. The Python code below provides an example of one round of such API calls. 

```{python eval=FALSE}
import re
from pyzillow.pyzillow import ZillowWrapper, GetDeepSearchResults

ZILLOW_ID = 'MY_ZID'
ZILLOW_API_KEY = 'MY_ZKEY'
zillow_data = ZillowWrapper(ZILLOW_API_KEY)

## Define function to acquire deep serach results from Zillow API
def get_zillow_price(address, zipcode, zillow_data=zillow_data):
    deep_search_response = zillow_data.get_deep_search_results(address, zipcode)
    result = GetDeepSearchResults(deep_search_response)
    return result

## For each row in listings, get data from Zillow
prices = []
for ix, row in LISTING_DF.iterrows():
    try:
        new_data = get_zillow_price(str(row['Address']), str(row['Zip']))
        print(str(row['Address']), str(row['Zip']))
        LISTING_DF.loc[ix, 'ZillowID'] = new_data.zillow_id
        LISTING_DF.loc[ix, 'Price'] = new_data.last_sold_price
        LISTING_DF.loc[ix, 'Latitude'] = new_data.latitude
        LISTING_DF.loc[ix, 'Longitude'] = new_data.longitude
        LISTING_DF.loc[ix, 'TaxValue'] = new_data.tax_value
        LISTING_DF.loc[ix, 'PropertySize'] = new_data.property_size
        LISTING_DF.loc[ix, 'YearBuilt'] = new_data.year_built
        LISTING_DF.loc[ix, 'HomeSize'] = new_data.home_size
        LISTING_DF.loc[ix, 'Bedrooms'] = new_data.bedrooms
        LISTING_DF.loc[ix, 'Bathrooms'] = new_data.bathrooms
        LISTING_DF.loc[ix, 'LastSold'] = new_data.last_sold_date
        print(LISTING_DF.loc[ix])
    except TypeError:
        new_data = get_zillow_price(str(row['Address']), '63119')
        print(str(row['Address']), str(row['Zip']))
        LISTING_DF.loc[ix, 'ZillowID'] = new_data.zillow_id
        LISTING_DF.loc[ix, 'Price'] = new_data.last_sold_price
        LISTING_DF.loc[ix, 'Latitude'] = new_data.latitude
        LISTING_DF.loc[ix, 'Longitude'] = new_data.longitude
        LISTING_DF.loc[ix, 'TaxValue'] = new_data.tax_value
        LISTING_DF.loc[ix, 'PropertySize'] = new_data.property_size
        LISTING_DF.loc[ix, 'YearBuilt'] = new_data.year_built
        LISTING_DF.loc[ix, 'HomeSize'] = new_data.home_size
        LISTING_DF.loc[ix, 'Bedrooms'] = new_data.bedrooms
        LISTING_DF.loc[ix, 'Bathrooms'] = new_data.bathrooms
        LISTING_DF.loc[ix, 'LastSold'] = new_data.last_sold_date
        print(LISTING_DF.loc[ix])
    except:
        # A few bad apples passed through
        pass
    
```


### Alcoholic Beverage Retailers, Restaurants & Bars
The Saint Louis community supports many brewers and liquor producers, and is also famous for its [food](https://www.timeout.com/chicago/restaurants/the-underrated-st-louis-food-scene-demands-a-weekend-getaway), arts and social scene. To capture a proxy for this enthusiasm for social interaction, data was acquired on all retail liquor licenses. The dataset was sourced via the [Missouri Alcohol Tobacco & Firearms](http://atc.dps.mo.gov/reports/) and accessed via the [data.mo.gov data portal](https://data.mo.gov/Regulatory/Missouri-Beer-Wine-and-Liquor-Solicitor-List-with-/mmn5-wy78). The data was pared down by selecting only the cities that are known to be in our dataset for home listings. Once acquired, the address, street, city, state and postal code were concatenated together in order to query [ggmap](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)'s library for their geocodes. Geocodes will be necessary in computing [Haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) for homes in the dataset, and are also necessary for visualizing both datasets on a map. 

The [R statistical computing language](https://www.r-project.org/) is used for the remaineder of this investigation.

```{r get_bars, eval=FALSE}
## The following code was used to clean and get geocodes for 
## the alcohol license dataset. This code chunk does not run
## to avoid calling the ggmap API unnecessarily
library(ggmap)

## Read in alcohol licenses
new_path = 'E:/Harvard Fundamentals of Data Science/Missouri_Primary_Alcohol_Licenses.csv'
alc_df = read.csv(new_path, header=TRUE)

## Reduce size of dataset by selecting cities/counties
CITIES_WE_HAVE = c("BALLWIN","BRENTWOOD","CHESTERFIELD","CLAYTON","CRESTWOOD",
           "CREVE COEUR","ELLISVILLE","FLETCHER","GLENDALE","KIRKWOOD",
           "LADUE","MANCHESTER","MARYLAND HEIGHTS","OAKLAND","RICHMOND HEIGHTS",
           "ROCK HILL","SAINT LOUIS","SHREWSBURY","SUNSET HILLS","TOWN AND COUNTRY",
           "UNIVERSITY CITY","WARSON WOODS","WEBSTER GROVES","WILDWOOD","WINCHESTER")
alc_df = alc_df[alc_df$CITY %in% CITIES_WE_HAVE, ]

msg = paste0('There are ', dim(alc_df)[1], ' wine/beer/spirits retailers in the area. Generating geocodes.'); print(msg)

## Put addresses into a format which ggmap can use to derive a geocode
addresses = paste0(alc_df$STREET.NUMBER, ' ', alc_df$STREET, ' ', alc_df$CITY, ' ',
                   alc_df$STATE, ' ', alc_df$ZIPCODE, ' ', 'USA')

## Get geocodes from ggmap API -- less than their API max (2500)
gcodes = geocode(addresses, output='latlon', source='google')
alc_df$LON = unlist(gcodes[1])
alc_df$LAT = unlist(gcodes[2])
head(alc_df)## NOW PRINT TO FILE WITH COMPLETE DATA

## Write to csv for later use and redundancy
write.csv(alc_df, 'E:/Harvard Fundamentals of Data Science/GEOCODED_STLCOUNTY_Primary_Alcohol_Licenses.csv')
write.csv(alc_df, 'C:/Users/paulm/Desktop/Harvard/Data/GEOCODED_STLCOUNTY_Primary_Alcohol_Licenses.csv')
```
<br>
The pared down, tidy dataset for the Saint Louis area's alcohol license holders appears below. Now that geocodes have been accessed we can explore what impact, if any, these purveyors of spirits have on their surrounding housing markets.
<br>
```{r}
## Read in alcohol licenses
new_path = 'E:/Harvard Fundamentals of Data Science/GEOCODED_STLCOUNTY_Primary_Alcohol_Licenses.csv'
# new_path = 'C:/Users/paulm/Desktop/Harvard/Data/GEOCODED_STLCOUNTY_Primary_Alcohol_Licenses.csv'
alc_df = read.csv(new_path, header=TRUE)
rownames(alc_df) = alc_df$PRIMARY.LICENSE
alc_df[, c('PRIMARY.LICENSEE','X')] = NULL
head(alc_df[, c('DBANAME','CITY','LAT','LON','PRIMARY.TYPE')]); dim(alc_df)
```
<br>
In the interest of feature engineering, the records in the liquor license dataset are aggregated up to the City level by a simple "count unique" function. This aggregated data is saved to a new namespace in order to preserve the individual locations for future geospatial analysis. It should be noted that no attempt was made to investigate the individual [liquor license types](http://atc.dps.mo.gov/licensing/faqs_alcohol.php) with any type of rigor, and thus their hetergoneity will not be captured in any forthcoming models.
<br>
```{r}
len_unique = function(x) length(unique(x))
alc_agg = aggregate(DBANAME ~ CITY, data=alc_df, FUN=len_unique)
names(alc_agg) = c('City','RetailLiquorLicenses')
head(alc_agg)
```


### Saint Louis County Municipalities

Data on Saint Louis County's various municipalities characteristics was acquired from the [Saint Louis County Municpality Wikipedia page](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwjXjrqXlMrTAhWFMGMKHUYsC84QFggqMAI&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FMunicipalities_of_St._Louis_County%2C_Missouri&usg=AFQjCNHjpFDCNArs8kQc5euPd4M6RB9gkg&sig2=vytcyQMAgG-HAIrLm9ad5Q) via simple copy/paste into a .csv document. The datset contains information on Population, Total Area (square miles), and population density of each municipality in the listings dataset. The following code segment does not run to minimize the calls to Google's API for geocode generation; the pre-geocoded dataset is silently read in for later use.
<br>
```{r, eval=FALSE}
## Do not re run this code
munis = read.csv('E:/Harvard Fundamentals of Data Science/STL County Municipality Data.csv',
                 header=TRUE)
## Get cities in muni data
cities_togcode = paste0(munis$Municipality, ' MO USA')

## Get geocodes from ggmap API -- less than their API max (2500)
gcodes = geocode(cities_togcode, output='latlon', source='google')
munis$LON = unlist(gcodes[1])
munis$LAT = unlist(gcodes[2])

## Coerce new data to numeric 
munis$Population = as.numeric(gsub(',','',munis$Population))
munis$Population.Density.sq.mi = as.numeric(gsub(',','',munis$Population.Density.sq.mi))

## Convert name to uppercase for future merges
munis$Municipality = toupper(munis$Municipality) 
```

```{r echo=FALSE}
## Read in result of above, conducted earlier
munis = read.csv('E:/Harvard Fundamentals of Data Science/STL County Municipality Data.csv',
                 header=TRUE)
head(munis)
```
<br>


## Data Munging, Joining & Feature Engineering

### Data Cleaning
The complete home listing dataset was cleaned by eliminating duplicate addresses, setting Address as the index, and purging observations that were missing values for Price, HomeSize, or YearBuilt. Originally the data that was filtered through the Zillow API contained 1777 observations; after the afforementioned process only 813 observations remain. This stringent purging of data reflects a bias towards reality, shying somewhat away from methods of data imputation.
  
```{r}
# new_path = 'C:/Users/paulm/Desktop/Harvard/COMBINED FINAL DATASET.csv'
new_path = 'E:/Harvard Fundamentals of Data Science/COMBINED FINAL DATASET.csv'
df = read.csv(new_path, header=TRUE)
obs_before = dim(df)[1]

## Remove poor quality data
out_1 = is.na(df$Price) == F
out_2 = is.na(df$HomeSize) == F
out_3 = is.na(df$YearBuilt) == F
df = df[out_1 & out_2 & out_3, ]
obs_after = dim(df)[1]
msg = paste0('Before there were ', 
             obs_before, ' records ... ',
             'After data cleanup there is ', 
             obs_after,' records.')
print(msg)

## Drop duplicates & Set index to minimize change of duplicates
df = df[duplicated(df$Address) == FALSE, ]
rownames(df) = df$Address
head(df[,c('ZillowID','Price','Bedrooms','Bathrooms','LastSold','HomeSize')])
```


### Joining Listings with Municipal Statistics

First we need to understand which cities in the residential real estate dataeset are spelled the same way (and present) in the municipality statistics dataset. Below, the City column in the listing data is converted to an uppercase character string to match the municipal statistics dataset's Municipality column. Then a list of all unique cities contained in the listings data is cross-referenced with the municipal statistics data to ensure that each city in the listings dataset (a) exists in the municipal dataset, and (b) is spelled the same. It is demonstrated below that both Saint Louis and Fletcher are not present in the municipal statistics dataset. Knowing this, the data is merged as a left join to preserve the 813 observations in the listing data. It should be noted that alcohol license information for the cities of Saint Louis and Fletcher will not be considered due to their absence. A truncated header of the dataset is shown below.
<br>

```{r}
## Convert listing cities to uppercase to match
df$City = toupper(df$City)
listcity_in_municity = c(unique(df$City) %in% unique(munis$Municipality))
names(listcity_in_municity) = unique(df$City)

## ID which cities are not present in dataset
cities_absent_munidata = listcity_in_municity[listcity_in_municity==FALSE]
print(cities_absent_munidata)

## Merge listings data with muni stats
df = merge(df, munis, 
               by.x='City', 
               by.y='Municipality',
               all.x=TRUE)

## Coerce new data to numeric 
df$Population = as.numeric(gsub(',','',df$Population))
df$Population.Density.sq.mi = as.numeric(gsub(',','',df$Population.Density.sq.mi))

head(df[,c('ZillowID','Price','Bedrooms','Population','Total.Area..mi2.')]); dim(df)
```
<br>

### Joining Listings with Retail Liquor License Summary

As above, the two City attributes are compared in order to ensure that data can be merged without misleading consumers of the analysis. It is shown that only cities in the 25 represented in the home listings dataset are not present in the retail alcohol license dataset. Encouragingly consistent with the previous merge, the datasets are merged on the City feature using a left merge. Following these operations, all columns that exist in the dataset are shown below.
<br>
```{r}
## Create list of unique cities in each source
list_cities = unique(df$City)
alc_cities = unique(alc_agg$City)

## Check that listing cities are in the alcohol data cities
listcity_in_alccity = c(list_cities %in% alc_cities)
names(listcity_in_alccity) = unique(df$City)

## ID which cities are not present in dataset
cities_absent_alcdata = listcity_in_alccity[listcity_in_alccity==FALSE]

print(paste0('Number of cities in listings data  ', length(listcity_in_alccity),
             ' out of which ', length(cities_absent_alcdata), 
             ' are not in the liquor data.'))
print(cities_absent_alcdata)

## Merge datasets on City
df = merge(df, alc_agg, by='City', all.x=TRUE)
print(head(df))
```


### Feature Extraction & Normalization

Now that we've enriched the listing data with municipal characteristics, we may now begin to extract basic features. This includes changing dates from strings to datetime objects to support mathematical operations, deriving ratios (e.g. PricePerSqFt), and preparing geographic information for use in various geospatial visualization packages.
<br>
```{r}
## Extract basic features
df$LastSold = as.Date(df$LastSold, '%m/%d/%Y')
df$PricePerSqFt = df$Price / df$HomeSize
df$YearsOld = as.numeric(format(Sys.Date(), '%Y')) - df$YearBuilt
df$YearsSinceLastSale = round(as.numeric(Sys.Date() - df$LastSold)/365, 2)
df$TaxValueToListPrice = df$TaxValue / df$Price
df$PricePerLotSqFt = df$Price / df$PropertySize
df$Coordinates = paste0('(',df$Latitude,',',df$Longitude,')')
df$HomeSizeSquared = df$HomeSize^2
df$YearsSinceLastSale_5OrLess = df$YearsSinceLastSale <= 5
```

As an exercise in model-building creativity, the top and bottom 5% of a range of features were transformed into categorical variables. The columns are added "inplace" by looping through various numeric features. The column names are printed for verification that they were created. 

```{r}
## Extract categories from continuous variables
cols_to_mark = list('Price','HomeSize','YearBuilt','Population',
                 'RetailLiquorLicenses','PricePerSqFt')
for(vector in cols_to_mark) {
  cutoffs = quantile(df[,vector], c(.05, .95), na.rm=TRUE)
  colname_low = paste0('Bottom5Percentile_',as.character(vector))
  colname_hi = paste0('Top5Percentile_',as.character(vector))
  ## Mark top and bottom 5%-iles
  df[,colname_low] = df[,vector] <= cutoffs[1]
  df[,colname_hi] = df[,vector] >= cutoffs[2]
}

## Print newest columns
head(df[,c(tail(seq_len(ncol(df))))], 2)
```
<br>
By running simple linear regression on the number of bedrooms, treating the Bedrooms column as a factor rather than a number, it aligns with expectations that homes with 5, 6 or 7+ bedrooms command higher prices than others. To isolate their signal, dummy variables were derived for homes greater than or equal to 5 bedrooms, as well as for each of the significant groups in the Bedroom set. It is interesting to note that the effect between 5 bedroom and 6 bedroom listings is roughly equal, yet homes with 7 bedrooms seem to pack a bigger punch. This may reflect [Saint Louis'](https://www.osv.com/osvnewsweekly/byissue/article/tabid/735/artmid/13636/articleid/9926/top-10-catholic-cities-usa.aspx) [strong Catholic tradition](http://stlouiscatholic.blogspot.com/) of large families, or may reflect the considerable [income inequality in the Saint Louis region.](https://fred.stlouisfed.org/series/2020RATIO029189) Further investigation would be necessary to validate these hypotheses, especially within the context of the housing market.
<br>
```{r}
## Explore isolated impact of bedrooms on price
mod = lm(df$Price ~ factor(df$Bedrooms))
print(summary(mod))

## These features were extracted using 
## what was learned in exploratory analysis
df$BedroomsFactor = factor(df$Bedrooms, levels=c(1,2,3,4,5,6,7))
df$FivePlusBedrooms = ifelse(df$Bedrooms >=5, TRUE, FALSE)
df$FiveBedrooms = ifelse(df$Bedrooms==5, TRUE, FALSE)
df$SixBedrooms = ifelse(df$Bedrooms==6, TRUE, FALSE)
df$SevenBedrooms = ifelse(df$Bedrooms==7, TRUE, FALSE)
df$ThreeOrMoreBaths = ifelse(df$Bathrooms >= 3, TRUE, FALSE)
```
<br>
The simple regression method of feature detection that was employed above on Bedrooms is also utilized to identify which municipalities currently command higher residential housing prices. This method indicates that the cities of Ladue, Richmond Heights, and Town and Country command higher prices. Maryland Heights lags behind, consistent with the fact that Maryland Heights is a highly industrial area compared with other, more residential municipalities in the area. This limited dataset is not sufficient to verify or deny any claims regarding any one municipality's residential real estate market. Information on other factors, as well as a larger sample of panel data would be a natural extension of this work.
<br>
```{r}
## Explore isolated impact of cities on price
mod = lm(df$Price ~ factor(df$City))
print(summary(mod))

df$IsLadue = ifelse(df$City=='LADUE', TRUE, FALSE)
df$IsTownAndCountry = ifelse(df$City=='TOWN AND COUNTRY', TRUE, FALSE)
df$IsRichmondHeights = ifelse(df$City=='RICHMOND HEIGHTS', TRUE, FALSE)
df$IsMarylandHeights = ifelse(df$City=='MARYLAND HEIGHTS', TRUE, FALSE)
```
<br>
Before normalizing the features, it is imperative to first understand something of the nature of each feature's underlying statistical distribution. These visualizations will help inform the method of normalization and data pre-processing that will be necessary for maximizing the probability of predictive success.
<br>
```{r fig.height=14}
library(gridExtra)

## Declare histogram function for re-use
check_histogram = function(col){
  xlab = col
  COL_MU = mean(df[,col], na.rm=T)
  mu_lab = paste0('mean = ', as.character(round(COL_MU, 1)))
  ggplot(data=df, aes(x=df[, col])) +
    geom_histogram(fill='lightgreen', colour='black', alpha=.6) +
    labs(title=paste0('Histogram of ', col), x=col) +
    geom_rug(alpha=.25) +
    geom_vline(xintercept=COL_MU, colour='blue', size=1.5) +
    geom_text(aes(x=COL_MU+(COL_MU*.7), y=50, label=mu_lab), size=6.4) +
    theme_minimal() +
    scale_x_continuous(labels=comma) + 
    scale_y_continuous(labels=comma)
}

## Visualize histograms
hist1 = check_histogram('Price')
hist2 = check_histogram('PricePerSqFt')
hist3 = check_histogram('HomeSize')
hist4 = check_histogram('PropertySize')
hist5 = check_histogram('YearsOld')
hist6 = check_histogram('YearsSinceLastSale')
hist7 = check_histogram('TaxValue')
hist8 = check_histogram('Bedrooms')
hist9 = check_histogram('Population')
hist10 = check_histogram('Population.Density.sq.mi')


grid.arrange(hist1,hist2,hist3,hist4,
             hist5,hist6,hist7,hist8,hist9,hist10, 
             ncol=2)
```
<br>
These histograms indicate that Price, HomeSize, TaxValue, and PropertySize are all skewed right, resembling more a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) than a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). To adjust for this phenomenon, the natural log of each of the afforementioned features is taken prior to normalization using a z-score method. 

Also gleaned from these visualizations is that PricePerSqFt and Bedrooms appear to be normally distributed, meaning it is safe to use a z-score normalization method without any pre-processing. Bedrooms can also be treated as an ordered factor, and both manifestations of the same feature were considered.

The empirical distributions for YearsOld, YearsSinceLastSale, Population, and Population.Density.sq.mi are not easily classifiable distributions, and thus will be scaled using a min-max method rather than over-fitting an empirically defined distribution function. Since there is some uncertainty here, YearsOld is normalized via two methods. Finally, YearsSinceLastSale resembles a Poisson distribution; thus, R's rpois() function was leveraged to scale this variable. The choice to evaluate this method was experimental.
<br>
```{r}
## Extract features based on distributions
df$LnPrice = log(df$Price)
df$LnPropertySize = log(df$PropertySize)
df$LnHomeSize = log(df$HomeSize)
df$LnTaxValue = log(df$TaxValue)

## Declare z-score scaler for re-use
scale_z = function(vector) {
  mu = mean(vector, na.rm=TRUE)
  sd = sd(vector, na.rm=TRUE)
  z = (vector - mu) / sd
  return(z)
}

## Normalize using z-score 
df$BedroomsNormalized = scale_z(df$Bedrooms)
df$LnPriceNormalized = scale_z(df$LnPrice)
df$PricePerSqFtNormalized = scale_z(df$PricePerSqFt)
df$PriceNormalized = scale_z(df$Price)
df$LnPropertySizeNormalized = scale_z(df$LnPropertySize)
df$LnHomeSizeNormalized = scale_z(df$LnHomeSize)
df$YearsOldNormalized = scale_z(df$YearsOld)
df$LnTaxValueNormalized = scale_z(df$LnTaxValue)

## Declare function for minmax normalization
scale_minmax = function(vector) {
  min = min(vector, na.rm=TRUE)
  max = max(vector, na.rm=TRUE)
  rng = max-min
  scl = (vector-min) / (rng-min)
  return(scl)
}

## Normalize using minmax
df$YearsSinceLastSaleMinMaxScaled = scale_minmax(df$YearsSinceLastSale)
df$HomeSizeMinMaxScaled = scale_minmax(df$HomeSize)
df$HomeSizeSquaredMinMaxScaled = scale_minmax(df$HomeSizeSquared)
df$PopulationMinMaxScaled = scale_minmax(df$Population)
df$PopulationDensityMinMaxScaled = scale_minmax(df$Population.Density.sq.mi)

## Test Poisson normalization - pure inquiry
mu_yrs = mean(ceiling(df$YearsSinceLastSale), na.rm=T)
df$YearsSinceLastSalePoissonNormalized = rpois(df$YearsSinceLastSale,
                                               lambda=mu_yrs)
```
<br>



# Exploratory Analysis

Visualizing the community on geographic coordinates is often helpful for context, and may lead to hypotheses to test down the line. R's [ggplot2 (Grammar of Graphics)](https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf) and [ggmap](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf) packages were used to plot the home listing dataset as well as the liquor license dataset in the same cartesian space. The listing data, represented by the black points, is scaled in size by PricePerSqFt, while the 2d density map represents price levels for the area. Quick analysis of this visualization confirms that the affluent areas of town (Clayton, Ladue, Webster Groves, et. al.) tend to have higher price levels.

```{r fig.width=10}
## Display visually on map using ggmap
library(ggmap)
mu_lat = mean(df$Latitude, na.rm=T)
mu_lon = mean(df$Longitude, na.rm=T)

mp = get_map(c(mu_lon, mu_lat), zoom=12, source='google',
             maptype='roadmap')
ggmap(mp) + 
  geom_point(data=df, alpha=0.6,
             aes(x=Longitude, y=Latitude, size=PricePerSqFt)) +
  stat_density2d(data=df, aes(x=Longitude, y=Latitude, size=Price,
                              fill=..level.., alpha=..level..), bins=12, 
                 geom='polygon') +
  scale_fill_gradient(low='red', high='green') +
  scale_alpha(range = c(0,.2), guide=FALSE) + 
  geom_point(data=alc_df, aes(x=LON, y=LAT), 
             shape='@', size=5, colour='blue', alpha=.5) +
  labs(title='Greater Saint Louis Housing Market, Spring 2017',
       x='Longitude', y='Latitude') +
  theme(legend.position='bottom')
  
```

The geospatial visualizations below are intended to help one gain a feel for the population and population density in the area. For visual clarity, the size of the text on the maps below is proportional to the value that is being displayed. 

```{r fig.height=6}
## Display visually on map using ggmap
library(gridExtra)
map1 = ggmap(mp) + 
  geom_point(data=munis, alpha=0.6, aes(x=LON, y=LAT, size=Population)) +
  scale_fill_gradient(low='white', high='red') +
  scale_alpha(range = c(0,.3), guide=FALSE) + 
  labs(title='Populations of Saint Louis Cities, Spring 2017',
       x='Longitude', y='Latitude') +
  theme(legend.position='none') +
  geom_text(data=munis, aes(x=LON, y=LAT, size=Population, label=Municipality),
            colour='blue1') 

map2 = ggmap(mp) + 
  geom_point(data=munis, alpha=0.6, aes(x=LON, y=LAT, size=Population.Density.sq.mi)) +
  scale_fill_gradient(low='white', high='red') +
  scale_alpha(range = c(0,.3), guide=FALSE) + 
  labs(title='Population Density of Saint Louis Cities, Spring 2017',
       x='Longitude', y='Latitude') +
  theme(legend.position='none') + 
  geom_text(data=munis, aes(x=LON, y=LAT, size=Population.Density.sq.mi, label=Municipality),
            colour='green4') 

grid.arrange(map1, map2, ncol=2)
```

To gain an overall sense of the relationships present in the dataset, it is helpful to visualize a scatterplot/correlation matrix. The scatterplot matrix below shows tight correlations between HomeSize and Price. Relatively strong relationships exist between PricePerSqFt & Price and PropertySize & Price. It is important to note that HomeSize and PropertySize are highly correlated, thus invalidating their simultaneous use in predicting Price. Going forward HomeSize will be considered dominant to PropertySize following the intuition that the size of a home is often a dominant the home buying decision. There appears to be a linear relation between the normalized Price and YearsSinceLastSale variables, however there does appear to be considerable noise around the relationship. Interestingly, there appears to be a strong relationship between PricePerSqFt and Price. It is also shown that properties in Ladue, as marked by IsLadue, tend to command higher Price and PricePerSqFt. The variables for population, population density, and retail liquor licenses do not appear to be significant, likely reflecting the lack of granularity of information that is inherent with merging aggregated data with disaggregated observations.

```{r fig.height=10}
library(GGally)
suspected_PC = c('LnPriceNormalized','LnHomeSizeNormalized','YearsOldNormalized',
                 'PricePerSqFtNormalized','PopulationMinMaxScaled','PopulationDensityMinMaxScaled',
                 'YearsSinceLastSaleMinMaxScaled','RetailLiquorLicenses','IsLadue')
df_num = df[, suspected_PC]
df_num$IsLadue = as.numeric(df_num$IsLadue)
ggpairs(df_num, title='Matrix of Relationships',
       upper=list(continuous='density', combo="box_no_facet"),
       lower=list(combo='facetdensity'), aes(color=IsLadue)) +
  theme_minimal()
```



To visualize the differences between cities across several columns at once, the compiled dataset is reconfigured using R's [reshape2](https://cran.r-project.org/web/packages/reshape2/reshape2.pdf) package. The data was melted into this format in order to take advantage of ggplot2's facet_wrap() function, which enables faceted visualizations to share their common axes, reducing visual clutter. The city of Ladue stands out again with large values for HomeSize, Price, and PricePerSqFt.

```{r fig.height=10}
## Get data in easier to visualize format
library(reshape2)
cols = c('Price','PricePerSqFt','HomeSize','YearsOld')
df_melt = melt(df, 'City', cols)

## Create boxplot function to compare variables between cities
ggplot(data=df_melt, aes(x=City, y=value)) +
  geom_boxplot(aes(fill=City)) +
  theme(legend.position='none',
        axis.text.x=element_text(angle=90, hjust=1)) +
  scale_y_continuous(labels=comma) +
  facet_wrap(~variable, ncol=2, scales='free_y') +
  labs(x='', y =NULL, title='Contrasting Features Between Cities')
```


Exploring the relationship previously uncovered between certain municipalities and list price, below visualizes the relationship between HomeSize and Price, using IsLadue (a binary variable indicating the listing's city is Ladue) as a dummy variable. The pre-normalized features are used to maintain interpretability and context. 

```{r, fig.height=6}
g = ggplot(data=df, aes(x=HomeSize, y=Price, size=YearsSinceLastSale,
                        group=IsLadue, label=City))
regr_plot = g +
  geom_point(aes(colour=IsLadue), alpha=0.4) +
  geom_smooth(aes(colour=IsLadue), method='lm', se=F) +
  scale_y_continuous(labels=dollar) +
  scale_x_continuous(labels=comma) +
  theme_minimal() +
  labs(title='Saint Louis Area Housing Prices vs. Home Size')
regr_plot
```

The final dataset to be used in machine learning is shown below.

```{r}
print(head(df))
```


# Machine Learning

Using the features generated up until this point, and guided by the insights gained from the exploratory analysis conducted, several machine learning models were tested on the data for their utility in predicting Price and PricePerSqFt, among other features. 

The first step to ensure integrity is to create a training dataset and a testing dataset by splitting the original dataset. A random seed is also set for reproducibility.

```{r}
## Set seed for reproducibility
set.seed(7)

## Specify 70% training, 30% testing
pct_train = 0.7
ix = NROW(df)
cutoff_ix = round(pct_train*ix,0)

## Generate random sorting column & sort
## to ensure we get as close to random sample as possible
df$SORTME = rnorm(1:ix, 500, 10000)
df = df[order(df$SORTME), ]
df$SORTME = NULL

train_df = df[1:cutoff_ix, ]
test_df = df[cutoff_ix:ix, ]
```


A natural place to start is with linear regression. It is somewhat obvious to use square footage as a predictor of price, however this model attempts to go a bit deeper by introducing two more variables. While the IsLadue variable is significant in determining HomeSize (a source of collinearity), the IsLadue variable was used in the model because the coefficient of variation of HomeSize ~ IsLadue is only about 7.4%. YearsSinceLastSale is tested as a predictor of Price in an attempt to capture the recent phenomenon of speculators remodeling their properties with the intent to sell at a profit. This variable is a proxy for the ["HGTV Effect"](http://www.thegoodhartgroup.com/the-hgtv-effect/). It is shown that all variables are significant at the 1% level. Further, each predictor was scrutinized for collinearity with the others, and only those that pass the test were kept in the model. The training model appears to be strong, showing an adjusted coefficient of determination of 62.19%. When analyzing the residuals, however, it appears that there is bias in the model due to the non-normal distributions present in the dataset.

```{r}
mod = lm(Price ~ LnHomeSizeNormalized + IsLadue + YearsSinceLastSaleMinMaxScaled, data=train_df)
summary(mod)
```

We see that the model performs well on the training data, achieving an adjusted coefficient of determination of 62.35%. Further, the p-value on each predictor is highly significant. Below is a crude visualization of the empirical distribution of the errors, showing that the residuals are biased considerably. This indicates that the model is not an ideal representation of the phenomena, and further investigation is needed. The source of this bias is likely  However this model is still a "good" model in the sense that it is useful as a lense for viewing the market. 

```{r fig.height=6}
qplot(mod$residuals, geom='histogram', 
      main='Residuals from Linear Regression Price = LnHomeSizeNormalized + IsLadue + YearSinceLastSaleMinMaxScaled',
      xlab='Residuals')
```

The testing dataset is used below to validate the model derived above. The same bias seems to appear in this dataset as in the training data, though less pronounced. The linear model also predicted many values that are negative, meaning the model is certainly not realistic as a pricing tool. This is likely due to the lack of signal coming from the large number of homes that occupy the lower end of the Price spectrum. It is likely that the noise identified in YearsSinceLastSale is causing miscalibration in the model, and in the future this variable will be dropped to test for sensitivity. At a high-level, it does appear that the model is robust in teh sense that the coefficient of determination is similar to the training model, coming in at 63.6%.

```{r}
keep_cols = c('LnHomeSizeNormalized','IsLadue','YearsSinceLastSaleMinMaxScaled')
test_df = test_df[complete.cases(test_df[,keep_cols])==T, ]
Y = test_df$Price
test_df = test_df[,keep_cols]
test_df$PredictedPrice = predict(mod, test_df)
test_df$ActualPrice = Y
test_df$Error = test_df$ActualPrice - test_df$PredictedPrice

one = ggplot(test_df, aes(x=Error)) +
  geom_histogram(colour='black', fill='lightyellow') +
  labs(title='Histogram of Test Data Errors') +
  theme_minimal() +
  scale_x_continuous(labels=comma)
two = ggplot(test_df, aes(x=PredictedPrice)) +
  geom_histogram(colour='black', fill='lightblue') +
  labs(title='Histogram of Predicted Price Values of Test Data') +
  theme_minimal() +
  scale_x_continuous(labels=comma)
grid.arrange(one, two, ncol=1)

r_sq = cor(test_df$ActualPrice, test_df$PredictedPrice)^2
print(paste0('R-squared for the testing model is ', round(r_sq, 4)))
```


To explore the available heuristics in the context of predicting PricePerSqFt, a conditional inference tree was explored. YearSinceLastSale was dropped for this model due to its inconsistent signal. It is interesting to note that Bathrooms emerges as a key differentiator, along with the IsLadue indicator. We see that homes in Ladue with more than 4.5 bathrooms command a much higher PricePerSqFt than others. It is interesting that the Bathrooms variable is more important than HomeSize, PropertySize, YearsOld, and Bedrooms. This model was explored but not employed due to its lack of interpretability and the limited sample used to train. In future elaborations, this model will be fed more data to better represent each leaf in the tree.

```{r, fig.height=10}
library(party)
FN = PricePerSqFt ~ HomeSize + IsLadue + PropertySize + YearsOld +
  Bedrooms + Bathrooms
tree = ctree(FN, data=df)
plot(tree, main='Conditional Inference Tree')
```




# Conclusions & Further Research
The pricing model derived using multiple regression appears to have utility and promise, and elaborating on both the dataset and exploring further the models available will likely prove fruitful. Given the limited sample size of this investigation it is safe to assume that any conclusions reached in this inquiry are incomplete. However the mechanics underlying Price seem to make intuitive sense, giving plenty of inspiration for further research. Further inquiry should include the creation of a database of all records investigated, panel data on the same properties over time, and if possible should include the actual price at which a given property wound up selling. 


# References

* Municipalities of Saint Louis, Missouri, Wikipedia https://en.wikipedia.org/wiki/Municipalities_of_St._Louis_County,_Missouri Accessed April 2017
* Realtor.com, https://realtor.com
* Zillow.com, https://zillow.com
* Zillow API https://www.zillow.com/howto/api/APIOverview.htm
* Leonard Richardson, BeautifulSoup, https://www.crummy.com/software/BeautifulSoup/ 
* BeautifulSoup via Stanford http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html 
* Hadley Wickham & David Kahle, ggmap https://cran.r-project.org/web/packages/ggmap/ggmap.pdf 
* Hadley Wickham, ggplot2 (Grammar of Graphics) https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf 
* pandas http://pandas.pydata.org/ 
* Gretchen R. Crowe, Top 10 Catholic Cities, USA, OSV Newsweekly https://www.osv.com/osvnewsweekly/byissue/article/tabid/735/artmid/13636/articleid/9926/top-10-catholic-cities-usa.aspx 5/22/2013
* Hadley Wickham, reshape2, https://cran.r-project.org/web/packages/reshape2/reshape2.pdf

