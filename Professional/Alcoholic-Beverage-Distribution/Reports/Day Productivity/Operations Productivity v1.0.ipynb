{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statewide Operations Productivity\n",
    "\n",
    "Semi-structured Excel files are automatically emailed to `paul.washburn@majorbrands.com` on the 15th and last day of each month.  The data contains all hours for each worker in the warehouse.\n",
    "\n",
    "**Note:  Negative records are filtered out of the data as they were not associated with valid `employee_id`s.  These records are few in number, but this must be understood from a high-level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "base_dir = 'C:/users/pmwash/Desktop/Re-Engineered Reports/Day Hours/'\n",
    "\n",
    "def drop_unnecessary_characters(str_list):\n",
    "    str_list = [str(s).lower().replace(' ', '_') for s in str_list]\n",
    "    str_list = [str(s).lower().replace('-_', '') for s in str_list]\n",
    "    return str_list\n",
    "\n",
    "def replace_unnamed_and_nans(col_list):\n",
    "    new_col_list = list()\n",
    "    for col in col_list:\n",
    "        col = str(col)\n",
    "        if '|nan' in col:\n",
    "            newcol = col.replace('|nan', '')\n",
    "            new_col_list.append(newcol)\n",
    "        elif 'unnamed:_' in col:\n",
    "            newcol = col.replace('unnamed:_', col_group)\n",
    "            newcol = ''.join(c for c in newcol if not c.isdigit())\n",
    "            new_col_list.append(newcol)\n",
    "        else:\n",
    "            new_col_list.append(col)\n",
    "            col_group = col.split('|')[0]\n",
    "    return new_col_list\n",
    "\n",
    "def adjust_roster_id(roster_emp_id):\n",
    "    fixed_id = [str(s)[:3] + str(s)[4:] for s in roster_emp_id]\n",
    "    return fixed_id\n",
    "\n",
    "def preprocess_hr_data(file_path):\n",
    "    '''Accepts path to the export from ADP from HR which is emailed\n",
    "    twice per month.'''\n",
    "    df = pd.read_csv(file_path, skiprows=8)\n",
    "    \n",
    "    # clean up column names\n",
    "    df.loc[0] = col_specifier = drop_unnecessary_characters(df.loc[0])\n",
    "    df.columns = drop_unnecessary_characters(df.columns)\n",
    "    col_list = [a +'|'+ b for a,b in zip(df.columns, col_specifier)]\n",
    "    df.columns = replace_unnamed_and_nans(col_list)\n",
    "    df.drop(index=0, inplace=True)\n",
    "    \n",
    "    # set data types to numeric after removing miscellaneous symbols\n",
    "    non_numeric_cols = ['labor_level_selected', 'employee_id', 'employee_name']\n",
    "    numeric_cols = [col for col in df.columns if col not in non_numeric_cols]\n",
    "    for col in numeric_cols:\n",
    "        # drop accounting symbols in currency\n",
    "        df[col] = df[col].str.replace('$', '')\n",
    "        df[col] = df[col].str.replace(',', '')\n",
    "        df[col] = df[col].str.replace('(', '-')\n",
    "        df[col] = df[col].str.replace(')', '')\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "    # capture date from the file name\n",
    "    dat = file_path.split('Worked ')[1]\n",
    "    df['starting_date'] = dt.strptime(dat.split(' - ')[0], '%m%d%Y')\n",
    "    \n",
    "    # set indices\n",
    "    df['year'] = df.starting_date.apply(lambda x: x.year)\n",
    "    non_numeric_cols = ['starting_date'] + non_numeric_cols\n",
    "    df.set_index(non_numeric_cols, inplace=True)\n",
    "    df.index = df.index.droplevel('employee_name') #drop names for privacy\n",
    "    \n",
    "    # map in semantics for labor level\n",
    "    labor_level_dict = {'/50/5220////' : 'Shipping Repacking',\n",
    "                        '/50/6502////' : 'Shipping Wages',\n",
    "                        '/50/6513////' : 'Shipping Casual', \n",
    "                        '/70/5220////' : 'Warehouse Repacking',\n",
    "                        '/70/7202////' : 'Warehouse Wages',\n",
    "                        '/70/7214////' : 'Warehouse Casual',\n",
    "                        '/70/7201////' : 'Warehouse Management'}\n",
    "    df['labor_level'] = df.index.get_level_values('labor_level_selected')\n",
    "    df.labor_level = df.labor_level.map(labor_level_dict)\n",
    "    df['month'] = dat_ix = df.index.get_level_values('starting_date')\n",
    "    df.month = df.month.apply(lambda d: format(d, '%B'))\n",
    "    df['pay_period'] = ['01' if str(d).split('-')[-1].split(' ')[0]=='01' else '02' for d in dat_ix]\n",
    "    df['month_period'] = df.month.astype(str) + '_' + df.pay_period.astype(str)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df['employee_id'] = df['employee_id'].astype(str)\n",
    "    df['employee_id'] = df['employee_id'].str.replace(' ', '').str.upper()\n",
    "    df['employee_id'] = adjust_roster_id(df['employee_id'])\n",
    "    \n",
    "    # drop negative values\n",
    "    df = df.loc[df['total|wages'] > 0]\n",
    "    df['night_crew'] = df['labor_level'].apply(lambda s: 'Shipping' in str(s))\n",
    "    \n",
    "    return df\n",
    "\n",
    "file_list = glob.glob(base_dir + '*.csv')\n",
    "ops_hours_df = pd.DataFrame()\n",
    "for file in file_list:\n",
    "    ops_hours_df = ops_hours_df.append(preprocess_hr_data(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fetch_operations_roster(fpath):\n",
    "    roster = pd.read_csv(fpath)\n",
    "    roster.columns = drop_unnecessary_characters(roster.columns)\n",
    "    roster.rename(columns={'position_id': 'employee_id'}, inplace=True)\n",
    "    roster['employee_id'] = roster['employee_id'].astype(str)\n",
    "    roster['employee_id'] = roster['employee_id'].str.replace(' ', '').str.upper()\n",
    "    roster.drop(columns=['first_name', 'last_name'], inplace=True)\n",
    "    return roster\n",
    "\n",
    "def merge_roster_with_adp(roster_fpath, ops_hours_df, verbose=1):\n",
    "    '''\n",
    "    Combines ADP data with Roster from HR.\n",
    "    '''\n",
    "    # read in roster data\n",
    "    roster_df = fetch_operations_roster(roster_fpath)\n",
    "\n",
    "    # merge with ops_hours_df\n",
    "    rows_before = ops_hours_df.shape[0]\n",
    "    ops_df = ops_hours_df.merge(roster_df, on='employee_id', how='left')\n",
    "    rows_after = ops_df.shape[0]\n",
    "\n",
    "    # check what got dropped\n",
    "    notinnewdata = ~ops_hours_df.employee_id.isin(ops_df.employee_id.tolist())\n",
    "    dropped_from_data = ops_hours_df.loc[notinnewdata, 'employee_id'].unique().tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        print('Roster Columns:')\n",
    "        print(roster_df.columns.tolist())\n",
    "        print('''\n",
    "        Merging in EMPLOYEE ROSTER data from HR.  \n",
    "\n",
    "        This file needs to be updated by hand each month from ADAM COLEMAN or whoever is in that role. \n",
    "\n",
    "        Rows before merging in Roster:    {}\n",
    "        Rows after merging in Roster:     {}\n",
    "\n",
    "        The following Employee IDs were dropped in this process:\n",
    "        {}\n",
    "        '''.format(rows_before, rows_after, dropped_from_data))\n",
    "\n",
    "    # add in which warehouse they are in\n",
    "    manager_dict = {'Manning, Travis': 'STL', \n",
    "                    'nan': 'STL', \n",
    "                    'Hercher, Donald': 'STL', \n",
    "                    'Coffer, Wesley': 'KC',\n",
    "                    'Surls, Kurtis': 'KC', \n",
    "                    'Ade, Richard': 'KC', \n",
    "                    'Jorgensen, Skylar': 'KC'}\n",
    "    ops_df['warehouse'] = ops_df.reports_to_name.map(manager_dict)\n",
    "    \n",
    "    return ops_df\n",
    "\n",
    "roster_fpath = base_dir + 'lookup data/operations_roster_03202018.csv'\n",
    "ops_df = merge_roster_with_adp(roster_fpath, ops_hours_df, verbose=0)\n",
    "#ops_df.set_index(['warehouse', 'reports_to_name', 'employee_id', 'month_period']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "def plot_by_month(df, y, title, \n",
    "               seperate_y_axis=False, \n",
    "               x_axis_label='', y_axis_label='', \n",
    "               scale='linear', initial_hide=False,\n",
    "               barmode='group'):\n",
    "    '''\n",
    "    Plot variables by month\n",
    "    '''\n",
    "    df = df.loc[:, y + ['month']].groupby('month')[y].sum()\n",
    "    \n",
    "    label_arr = list(df)\n",
    "    series_arr = list(map(lambda col: df[col], label_arr))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: round(x, 2))\n",
    "    print(title); print('-' * 100); print(df.T)\n",
    "    \n",
    "    \n",
    "    layout = go.Layout(\n",
    "        barmode = barmode,\n",
    "        title = title,\n",
    "        legend = dict(orientation=\"h\"),\n",
    "        xaxis = dict(type='month',\n",
    "                  title='Month'),\n",
    "        yaxis=dict(\n",
    "            title = y_axis_label,\n",
    "            showticklabels = not seperate_y_axis,\n",
    "            type = scale\n",
    "        ),\n",
    "        bargap=0.2\n",
    "    )\n",
    "    \n",
    "    y_axis_config = dict(\n",
    "        overlaying = 'y',\n",
    "        showticklabels = False,\n",
    "        type = scale )\n",
    "    \n",
    "    visibility = 'visible'\n",
    "    if initial_hide:  visibility = 'legendonly'\n",
    "        \n",
    "    # make a trace for each series\n",
    "    trace_arr = []\n",
    "    for index, series in enumerate(series_arr):\n",
    "        trace = go.Bar( #go.Scatter\n",
    "            x=series.index, \n",
    "            y=series, \n",
    "            text=title, \n",
    "            name=label_arr[index],\n",
    "            visible=visibility,\n",
    "            opacity=0.7\n",
    "        )\n",
    "        # Add seperate axis for the series\n",
    "        if seperate_y_axis:\n",
    "            trace['yaxis'] = 'y{}'.format(index + 1)\n",
    "            layout['yaxis{}'.format(index + 1)] = y_axis_config    \n",
    "        trace_arr.append(trace)\n",
    "    fig = go.Figure(data=trace_arr, layout=layout)\n",
    "    py.iplot(fig)\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "def interactive_bar_plot(ops_df, y_list, subgroup, y_axis_label='Total Wages ($)'):\n",
    "    for grp, df in ops_df.groupby(['warehouse', subgroup]):\n",
    "        title = str(grp).replace('(', '').replace(')', '').replace('\\'', '')\n",
    "        plot_by_month(df, \n",
    "                      y=y_list, \n",
    "                      title=title, \n",
    "                      seperate_y_axis=False, \n",
    "                      y_axis_label=y_axis_label, \n",
    "                      scale='linear',\n",
    "                      initial_hide=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "def generate_calendar(year):\n",
    "    from pandas.tseries.offsets import YearEnd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    \n",
    "    start_date = pd.to_datetime('1/1/'+str(year))\n",
    "    end_date = start_date + YearEnd()\n",
    "    DAT = pd.date_range(str(start_date), str(end_date), freq='D')\n",
    "    WK = [d.strftime('%U') for d in DAT]\n",
    "    MO = [d.strftime('%B') for d in DAT]\n",
    "    holidays = USFederalHolidayCalendar().holidays(start=start_date, end=end_date)\n",
    "\n",
    "    DAYZ = pd.DataFrame({'Date':DAT, 'WeekNumber':WK, 'Month':MO})\n",
    "    \n",
    "    DAYZ['Year'] = [format(d, '%Y') for d in DAT]\n",
    "    DAYZ['Weekday'] = [format(d, '%A') for d in DAT]\n",
    "    DAYZ['DOTM'] = [format(d, '%d') for d in DAT]\n",
    "    DAYZ['IsWeekday'] = DAYZ.Weekday.isin(['Monday','Tuesday','Wednesday','Thursday','Friday'])\n",
    "    DAYZ['IsProductionDay'] = DAYZ.Weekday.isin(['Tuesday','Wednesday','Thursday','Friday'])\n",
    "    last_biz_day = [str(format(dat, '%Y-%m-%d')) for dat in pd.date_range(start_date, end_date, freq='BM')]\n",
    "    DAYZ['LastSellingDayOfMonth'] = [dat in last_biz_day for dat in DAYZ['Date'].astype(str)]\n",
    "\n",
    "    DAYZ.loc[DAYZ.WeekNumber.isin(['00','01','02','03','04','05','06','07','08','09','50','51','52','53']), 'Season'] = 'Winter'\n",
    "    DAYZ.loc[DAYZ.WeekNumber.isin(['10','11','12','13','14','15','16','17','18','19','20','21','22']), 'Season'] = 'Spring'\n",
    "    DAYZ.loc[DAYZ.WeekNumber.isin(['23','24','25','26','27','28','29','30','31','32','33','34','35']), 'Season'] = 'Summer'\n",
    "    DAYZ.loc[DAYZ.WeekNumber.isin(['36','37','38','39','40','41','42','43','44','45','46','47','48','49']), 'Season'] = 'Autumn'\n",
    "    DAYZ['Holiday'] = DAYZ.Date.isin(holidays)\n",
    "    DAYZ['HolidayWeek'] = DAYZ['Holiday'].rolling(window=7,center=True,min_periods=1).sum()\n",
    "    DAYZ['ShipWeek'] = ['A' if int(wk) % 2 == 0 else 'B' for wk in WK]\n",
    "\n",
    "    DAYZ.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return DAYZ\n",
    "\n",
    "\n",
    "def as400_date(dat):\n",
    "    '''Accepts list of dates as strings from theAS400'''\n",
    "    return [pd.to_datetime(dt.date(dt.strptime(d[-6:], '%y%m%d'))) for d in dat]\n",
    "\n",
    "def order_fulfillment_rate(all_files, year=2018, verbose=0):\n",
    "    '''Reads MTC1 via pw_custseg query files stored on local \n",
    "    machine to extract order fulfillment rate'''\n",
    "    \n",
    "    DF_OUT = pd.DataFrame()\n",
    "    for file in all_files:\n",
    "        # Specify datatypes from start to avoid issues downstream\n",
    "        if verbose: print('Reading file:\\n %s' %file); print('-'*100)\n",
    "        dtypes = col_names = {'#MCUS#':str,'#MIVDT':str,'#MIVND':str,'#MLIN#':str,'#MPRD#':str,'#MQTYS':np.int64,\n",
    "                    'CSCRDT':str,'CCRLIM':np.float64,'CONPRM':str,'CUSPMC':str,'CDDAY':str,\n",
    "                    '#MEXT$':np.float64,'CTERM@':str,'#MCOS$':np.float64,'CSTOR#':str,'#MCHN#':str,'#MCUSY':str,\n",
    "                    '#MSLSP':str,'#MQPC':np.int64,'#MCLA@':str,'#MSIZ@':str,'#MBRND':str,'#MQTY@':str,\n",
    "                    '#MCMP':str,'#MSUPL':str,'#MCALL':str,'#MPRIO':str,'#MINP#':str,\n",
    "                    '#CSTDTE':str,'CUDSCC':str,'CSHP':str,'CADMBR':str,'#MQTYO':np.float64,'#MTRCD':str,'#MCMRC':str}\n",
    "        c = pd.read_csv(file, header=0, dtype=dtypes)\n",
    "\n",
    "        ## Rename columns to make sense\n",
    "        col_names = {'#MCUS#':'CustomerId','#MIVDT':'Date','#MIVND':'Invoice','#MLIN#':'Line','#MPRD#':'SupplierBrandSizeNumber','#MQTYS':'QuantitySold',\n",
    "                    'CSCRDT':'SeasonCreditLimit','CCRLIM':'CreditLimit','CONPRM':'OnPremise','CUSPMC':'MerchandiseClass','CDDAY':'X2',\n",
    "                    '#MEXT$':'Revenue','CTERM@':'TermsCode','#MCOS$':'Cost','CSTOR#':'BarChainCode','#MCHN#':'ChainId','#MCUSY':'CustomerType',\n",
    "                    '#MSLSP':'SalespersonId','#MQPC':'QPC','#MCLA@':'ClassCode','#MSIZ@':'SizeCode','#MBRND':'BrandId','#MQTY@':'QtyCode',\n",
    "                    '#MCMP':'Warehouse','#MSUPL':'SupplierId','#MCALL':'CallCode','#MPRIO':'Priority','#MINP#':'ProductId','#MPRM@':'X1',\n",
    "                    'CSTDTE':'CustomerSetup','CUDSCC':'DisplayCaseClass','CSHP':'Ship','CADMBR':'ShipWeekPlan','#MQTYO':'QuantityOrdered',\n",
    "                    '#MTRCD':'TransactionCode','#MCMRC':'CreditReasonCode'}\n",
    "        c.rename(columns=col_names, inplace=True)\n",
    "        c.drop(labels=['X1','X2'], axis=1, inplace=True)\n",
    "\n",
    "        ## Extract Invoice & Line\n",
    "        c['InvoiceLine'] = [str(a)+'_'+str(b) for a,b in zip(c.Invoice, c.Line)]\n",
    "\n",
    "        ## Extract proper dates and derivative data\n",
    "        c.Date = as400_date(c.Date)\n",
    "        \n",
    "        ## Extract Cases \n",
    "        CS, QPC = c['QuantitySold'].astype(np.float64), c['QPC'].astype(np.float64)\n",
    "        BTLS = c['QuantitySold'].astype(np.float64)\n",
    "        c['Cases'] = np.divide(CS, QPC)\n",
    "        c['Bottles'] = BTLS\n",
    "        \n",
    "        ## Cases short\n",
    "        c['OutOfStock'] = np.int64(c['QuantitySold'].astype(np.float64) == 0)\n",
    "        OOS_CR_REASON = c['CreditReasonCode'] == '17'\n",
    "        TRC_A = c['TransactionCode'] == 'A'\n",
    "        c.loc[OOS_CR_REASON & TRC_A, 'OutOfStock'] = 1\n",
    "        Q_ORDERED = c['QuantityOrdered'].astype(np.float64)\n",
    "        c['CasesOrdered'] = np.divide(Q_ORDERED, QPC)\n",
    "        c.loc[c['OutOfStock']==1, 'CasesShort'] = np.subtract(c.loc[c['OutOfStock']==1, 'CasesOrdered'].astype(np.float64), c.loc[c['OutOfStock']==1, 'Cases']).astype(np.float64)## OOS mark\n",
    "         \n",
    "        ## Label customer types, call codes, class codes & warehouse\n",
    "        type_map = {'A':'Bar/Tavern','C':'Country Club','E':'Transportation/Airline','G':'Gambling',\\\n",
    "                        'J':'Hotel/Motel','L':'Restaurant','M':'Military','N':'Fine Dining','O':'Internal',\\\n",
    "                        'P':'Country/Western','S':'Package Store','T':'Supermarket/Grocery','V':'Drug Store',\\\n",
    "                        'Y':'Convenience Store','Z':'Catering','3':'Night Club','5':'Adult Entertainment','6':'Sports Bar',\\\n",
    "                        'I':'Church','F':'Membership Club','B':'Mass Merchandiser','H':'Fraternal Organization',\\\n",
    "                        '7':'Sports Venue'}\n",
    "        c.CustomerType = c.CustomerType.map(type_map)\n",
    "        call_codes = {'01':'Customer Call','02':'ROE/EDI','03':'Salesperson Call','04':'Telesales','BH':'Bill & Hold',\n",
    "                     'BR':'Breakage','CP':'Customer Pickup','FS':'Floor Stock','HJ':'High Jump','KR':'Keg Route',\n",
    "                     'NH':'Non-Highjump','NR':'Non-Roadnet','PL':'Pallets','PR':'Personal','RB':'Redbull',\n",
    "                     'SA':'Sample','SP':'Special','WD':'Withdrawal'}\n",
    "        c.CallCode = c.CallCode.map(call_codes)\n",
    "        product_class_map = {'10':'Liquor', '25':'Spirit Coolers', '50':'Wine', '51':'Fine Wine', '53':'Keg Wine',\n",
    "                                '55':'Sparkling Wine & Champagne', '58':'Package Cider', '59':'Keg Cider', '70':'Wine Coolers',\n",
    "                                '80':'Malt Coolers/3.2 Beer', '84':'High-Alcohol Malt', '85':'Beer', '86':'Keg Beer', \n",
    "                                '87':'Keg Beer w/ Deposit', '88':'High Alcohol Kegs', '90':'Water/Soda', '91':'Other Non-Alcoholic',\n",
    "                                '92':'Red Bull', '95':'Taxable Items - On Premise', '99':'Miscellaneous'}\n",
    "        c.ClassCode = c.ClassCode.map(product_class_map)\n",
    "        c['WarehouseCrossdock'] = c.Warehouse.map({'1':'KC','2':'STL','3':'COL','5':'SPFD'})\n",
    "        c['Warehouse'] = c.Warehouse.map({'1':'KC','2':'STL','3':'STL','5':'KC'})\n",
    "        \n",
    "        ## Merge with calendar\n",
    "        calendar = pd.DataFrame()\n",
    "        for yr in [2016,2017,2018]:\n",
    "            calnew = generate_calendar(year=yr)\n",
    "            calendar = calendar.append(calnew)\n",
    "        c = c.merge(calendar, on='Date', how='left')\n",
    "        c['DOM'] = c.Date.apply(lambda x: x.day)\n",
    "        \n",
    "        ## Change bill & hold dates to next month\n",
    "        incr_by_5 = lambda d: d + datetime.timedelta(days=5)\n",
    "        is_bill_n_hold = c.CallCode == 'Bill & Hold'\n",
    "        c.loc[is_bill_n_hold, 'Date'] = c.loc[is_bill_n_hold, 'Date'].apply(incr_by_5)\n",
    "        c.loc[is_bill_n_hold, 'Month'] = c.loc[is_bill_n_hold, 'Date'].apply(lambda d: dt.strftime(d, '%B'))\n",
    "        c.loc[is_bill_n_hold, 'Year'] = c.loc[is_bill_n_hold, 'Date'].apply(lambda d: d.year)\n",
    "        \n",
    "        ## Append new data to a dataframe that compiles all of it\n",
    "        DF_OUT = DF_OUT.append(c)\n",
    "\n",
    "    return DF_OUT\n",
    "\n",
    "# Gets pw_custseg query which is mass query of MTC1\n",
    "path = 'C:\\\\Users\\\\pmwash\\\\Desktop\\\\Re-Engineered Reports\\\\Customer Segmentation\\\\Data\\\\*.csv'\n",
    "all_files = glob.glob(path)\n",
    "\n",
    "# specify which months to grab to match above\n",
    "months_in_files = [str(s).split('\\\\pw_custseg ')[1].split('.csv')[0] for s in all_files]\n",
    "desired_months = ['12-2017', '01-2018', '02-2018']\n",
    "selected_files = [all_files[f] for f in np.arange(len(all_files)) if months_in_files[f] in desired_months]\n",
    "\n",
    "# run function defined above\n",
    "mtc_df = order_fulfillment_rate(selected_files)\n",
    "# mtc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def combine_ops_and_mtc(ops_df, mtc_df, net_cases=False):\n",
    "    '''Aggregates by month both dataframes then merges them on\n",
    "    warehouse, year and month in order to derive CPMH at various levels\n",
    "    \n",
    "    Still need to do it by type (i.e. warehouse hours)\n",
    "    '''\n",
    "    # preprocess\n",
    "    if net_cases:  mtc_df = mtc_df.loc[mtc_df.Cases > 0]\n",
    "    NO_SPACES = lambda s: str(s).replace(' ', '')\n",
    "    mtc_df.Warehouse = mtc_df.Warehouse.apply(NO_SPACES)\n",
    "    mtc_df.Year = mtc_df.Year.apply(NO_SPACES)\n",
    "    ops_df.warehouse = ops_df.warehouse.apply(NO_SPACES)\n",
    "    ops_df.year = ops_df.year.apply(NO_SPACES)\n",
    "    \n",
    "    # process mtc_df so it is aggregated by month\n",
    "    grpcols_mtc = ['Year','Warehouse','Month']\n",
    "    bymonth_mtc_df = pd.DataFrame(mtc_df.groupby(grpcols_mtc)[['Cases', 'Revenue']].sum())\n",
    "    \n",
    "    # process ops_df so it is aggregated by month\n",
    "    # first do for day crew\n",
    "    grpcols_ops = ['year','warehouse','month']\n",
    "    agg_cols = ['overtime|hours', 'regular|hours', 'total|hours']\n",
    "    ops_dfx = ops_df.loc[ops_df.night_crew==False]\n",
    "    bymonth_ops_df = ops_dfx.groupby(grpcols_ops)[agg_cols].sum()\n",
    "    bymonth_ops_df.index.rename(['Year', 'Warehouse', 'Month'], inplace=True)\n",
    "    \n",
    "    # first merged dataframe creation\n",
    "    # merge the dataframes on warehouse, month and year\n",
    "    bymonth_df = bymonth_ops_df.join(bymonth_mtc_df, how='left')\n",
    "    proc_str = lambda col: str(col).lower().replace('|', '_')\n",
    "    bymonth_df.columns = [proc_str(col) for col in bymonth_df.columns]\n",
    "    \n",
    "    # derive cpmh for day crew\n",
    "    bymonth_df.rename(columns={'regular_hours': 'day_regular_hours', \n",
    "                              'overtime_hours': 'day_overtime_hours',\n",
    "                              'total_hours': 'day_total_hours'}, inplace=True)\n",
    "    bymonth_df['day_cpmh_adjusted'] = np.divide(bymonth_df['cases'], \n",
    "                                                bymonth_df['day_regular_hours'])\n",
    "    \n",
    "    # do it again for night crew\n",
    "    # process ops_df so it is aggregated by month\n",
    "    ops_dfx = ops_df.loc[ops_df.night_crew==True]\n",
    "    ops_dfx = ops_dfx.groupby(grpcols_ops)[agg_cols].sum()\n",
    "    ops_dfx.rename(columns={'regular|hours': 'night_regular_hours', \n",
    "                            'overtime|hours': 'night_overtime_hours',\n",
    "                            'total|hours': 'night_total_hours'}, inplace=True)\n",
    "    ops_dfx.index.rename(['Year', 'Warehouse', 'Month'], inplace=True)\n",
    "    # adding second merge to combined dataframe\n",
    "    bymonth_df = bymonth_df.join(ops_dfx) # merge 2\n",
    "    \n",
    "    # derive cpmh for night crew\n",
    "    bymonth_df['night_cpmh_adjusted'] = np.divide(bymonth_df['cases'], \n",
    "                                                  bymonth_df['night_regular_hours'])\n",
    "    \n",
    "    # do it one more time for combined\n",
    "    # process ops_df so it is aggregated by month\n",
    "    ops_dfx = ops_df.groupby(grpcols_ops)[agg_cols].sum()\n",
    "    ops_dfx.rename(columns={'regular|hours': 'overall_regular_hours', \n",
    "                            'overtime|hours': 'overall_overtime_hours',\n",
    "                            'total|hours': 'overall_total_hours'}, inplace=True)\n",
    "    ops_dfx.index.rename(['Year', 'Warehouse', 'Month'], inplace=True)\n",
    "    # adding second merge to combined dataframe\n",
    "    bymonth_df = bymonth_df.join(ops_dfx) # merge 2\n",
    "    \n",
    "    # derive cpmh for night crew\n",
    "    bymonth_df['overall_cpmh_adjusted'] = np.divide(bymonth_df['cases'], \n",
    "                                                    bymonth_df['overall_regular_hours'])\n",
    "    \n",
    "    # make ix lowercase for plotting\n",
    "    bymonth_df.index.names = [str(s).lower() for s in bymonth_df.index.names]\n",
    "    bymonth_df.reset_index(inplace=True, drop=False)\n",
    "    #bymonth_df.set_index('year', inplace=True)\n",
    "    \n",
    "    return bymonth_df\n",
    "\n",
    "bymonth_df = combine_ops_and_mtc(ops_df, mtc_df)\n",
    "# bymonth_df.head()\n",
    "\n",
    "# specify parameters\n",
    "subgroup = 'year'\n",
    "y_list = ['day_cpmh_adjusted', 'night_cpmh_adjusted', 'overall_cpmh_adjusted']\n",
    "\n",
    "print('''\n",
    "CPMH by Warehouse/Month\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(bymonth_df, y_list, subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "-----------------------------------------------------------\n",
    "\n",
    "# *Peripheral Information*\n",
    "\n",
    "Information shown below is non-central to the intended purpose of this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "-----------------------------------------------------------\n",
    "# Total Employee Wages \n",
    "\n",
    "Below wages are plotted by various categories.  Numbers shown are summed by group and month to show changes over time.  \n",
    "\n",
    "## Wages by Warehouse & Day/Night\n",
    "\n",
    "`True` indicates \"Night Crew\", `False` indicates \"Day Crew.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "subgroup = 'night_crew'\n",
    "y_list = ['doubletime|wages', 'overtime|wages', 'regular|wages', 'total|wages']\n",
    "\n",
    "print('''\n",
    "Wages ($) by Warehouse & Day/Night Cre\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(ops_df, y_list, subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "## Wages by Warehouse & Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "subgroup = 'reports_to_name'\n",
    "y_list = ['doubletime|wages', 'overtime|wages', 'regular|wages', 'total|wages']\n",
    "\n",
    "print('''\n",
    "Wages ($) by Warehouse/Manager\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(ops_df, y_list, subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "## Wages by Warehouse & Labor Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "subgroup = 'labor_level'\n",
    "y_list = ['doubletime|wages', 'overtime|wages', 'regular|wages', 'total|wages']\n",
    "\n",
    "print('''\n",
    "Wages ($) by Warehouse/Labor Level\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(ops_df, y_list, subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "-----------------------------------------------------------\n",
    "\n",
    "# Total Employee Hours \n",
    "\n",
    "The plots below reflect same plotting method as was employed above, only it is plotting employee hours on the y-axis.    \n",
    "\n",
    "## Hours by Warehouse & Day/Night\n",
    "\n",
    "`True` indicates \"Night Crew\", `False` indicates \"Day Crew.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "subgroup = 'night_crew'\n",
    "y_list = ['doubletime|hours', 'overtime|hours', 'regular|hours', 'total|hours']\n",
    "\n",
    "print('''\n",
    "Hours by Warehouse/Labor Level\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(ops_df, y_list, subgroup, y_axis_label='Total Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "## Hours by Warehouse & Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify parameters\n",
    "subgroup = 'labor_level'\n",
    "y_list = ['doubletime|hours', 'overtime|hours', 'regular|hours', 'total|hours']\n",
    "\n",
    "print('''\n",
    "Hours by Warehouse/Labor Level\n",
    "''')\n",
    "\n",
    "interactive_bar_plot(ops_df, y_list, subgroup, y_axis_label='Total Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "source": [
    "# Appendix & Technical Notes\n",
    "\n",
    "## About the Data\n",
    "\n",
    "Data for this report, specifically the HR data, did not start coming in until February 2018.  **This means that only information from February 2018 forward will be shown.**  Due to lack of structured processing and organization of data in the past we do not have a history available unless ADP can provide it.  Given we switched HR system providers in 2017 this is unlikely.  \n",
    "\n",
    "The data for this report is sourced from the following:\n",
    "\n",
    "- Summary data from ADP is emailed from HR bi-monthly (1st and 15th) to summarize all operations hours.\n",
    "- A Roster is merged in to match managers and thus warehouse information.\n",
    "- Following precedent (i.e. the old version of this report), Non-Standard Cases are used.  If the Daily Report is ever systemitized then we can use Split Cases instead (if desired).\n",
    "- The query `pw_custseg` is used to access `MTC1` data.  This needs to be updated before running this report.\n",
    "\n",
    "Names are dropped from all data sets to protect employee privacy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from math import ceil as roundup\n",
    "# %matplotlib inline\n",
    "\n",
    "# def plot_timeseries_by_category(ops_df, x, y='total|wages', category='labor_level', \n",
    "#                      sub_category='warehouse', suptitle=None, verbose=0):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     all_categories = ops_df[category].unique()\n",
    "#     n_levels = len(all_categories)\n",
    "    \n",
    "#     if verbose:\n",
    "#         print('''\n",
    "#         There are {} levels in this category:\n",
    "#         {}\n",
    "#         '''.format(n_levels, all_levels))\n",
    "\n",
    "#     n_rows = roundup(n_levels/2)\n",
    "#     fig, axes = plt.subplots(nrows=n_rows, ncols=2, figsize=(15, 6*n_rows), sharex=False)\n",
    "\n",
    "#     for i, cat in enumerate(all_categories):\n",
    "#         _df = ops_df.loc[ops_df[category] == cat]\n",
    "#         _df =  _df.groupby([sub_category, x])[y].sum().reset_index(drop=False)\n",
    "\n",
    "#         all_subcategories = _df[sub_category].unique()\n",
    "        \n",
    "#         for subcat in all_subcategories:\n",
    "#             _df = _df.loc[_df[sub_category] == subcat]\n",
    "#             N = np.arange(len(_df[x].unique()))\n",
    "            \n",
    "#             if _df.shape[0] == 0:\n",
    "#                 pass\n",
    "#             elif i < n_rows:\n",
    "#                 _df.plot(x, y, kind='barh', ax=axes[i, 0])\n",
    "#                 #axes[i, 0].set_xticks(N, _df[x])\n",
    "#                 axes[i, 0].grid(alpha=.7)\n",
    "#                 axes[i, 0].set_xlabel('Total Wages')\n",
    "#                 axes[i, 0].set_ylabel('')\n",
    "#                 axes[i, 0].set_title(subcat)\n",
    "#                 axes[i, 0].legend(loc='best')\n",
    "#             else:\n",
    "#                 _df.plot(x, y, kind='barh', ax=axes[i-n_rows, 1])\n",
    "#                 axes[i-n_rows, 1].grid(alpha=.7)\n",
    "#                 axes[i-n_rows, 1].set_xlabel('Total Wages')\n",
    "#                 axes[i-n_rows, 1].set_ylabel('')\n",
    "#                 axes[i-n_rows, 1].set_title(subcat)\n",
    "#                 axes[i-n_rows, 1].legend(loc='best')\n",
    "#         sns.despine()\n",
    "        \n",
    "#     if suptitle != None: plt.suptitle(suptitle)\n",
    "    \n",
    "#     return None\n",
    "        \n",
    "# plot_by_category(ops_df, \n",
    "#                  x='month',\n",
    "#                  y='total|wages', \n",
    "#                  category='labor_level', \n",
    "#                  sub_category='warehouse', \n",
    "#                  suptitle='Hours v. Wages by Manager')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
