{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paul M. Washburn\n",
    "\n",
    "# Delivery Analysis\n",
    "\n",
    "This analysis is being undertaken with the over arching goal of finding out whether dropping a truck lease is possible.  The first analysis looks at Saint Louis and Columbia only.  If warranted we will look at Kansas City and Springfield later.\n",
    "\n",
    "# Data \n",
    "\n",
    "There are three sources of data used in this inquiry.  \n",
    "\n",
    "- The Production Tab of the Daily Report\n",
    "- Data extract from Hogan containing all of our invoice transactions since 2016 (through April 2018)\n",
    "- Internal accounting data from the general ledger on Hogan invoices\n",
    "\n",
    "\n",
    "# Goals\n",
    "\n",
    "- Characterize the history of deliveyr patterns before/after Schlafly\n",
    "- Learn about delivery patterns across houses\n",
    "- Characterize rental costs since 2016 \n",
    "- Simulate a likely future scenario and account for rental costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_rows', 99999999)\n",
    "pd.set_option('max_columns', 99999999)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Production Tab of Daily Report 2016-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_stl(file, year):\n",
    "    '''Takes date from file name'''\n",
    "    regex_criteria = re.compile(r'[0-9]+-[0-9]+')    \n",
    "    dat = re.findall(regex_criteria, file)\n",
    "    exclude = set(string.punctuation)\n",
    "    dat = ''.join(d for d in dat if d not in exclude)\n",
    "    dat = str(dat + '-' + year)\n",
    "    dat = dt.strptime(str(dat), \"%m-%d-%Y\").date()\n",
    "    return dat\n",
    "\n",
    "def extract_stl_production_tab(file, year):\n",
    "    '''\n",
    "    Takes in and formats Production Tab from Daily Report. \n",
    "    Extracts date from filename and creates index.\n",
    "    Puts into a dictionary of dataframes \n",
    "    for input into a pandas DataFrame.\n",
    "    '''\n",
    "    dtypes = {'Date':dt.date, 'Warehouse':str,'LOC':str,'RTE':str,'Driver':str,'Truck#':str,\n",
    "            'Stops':np.float64,'TTL Cs/splt':np.float64,'Cs':np.float64,'Btls':np.float64,\n",
    "            'Start Hr':str, 'End Hr':str,'Ttl Hrs':str,'Ttl Mi':np.float64 }\n",
    "    try:\n",
    "        df = pd.read_excel(file, sheet_name='Production', converters=dtypes)\n",
    "    except ValueError:\n",
    "        df = pd.read_excel(file, sheet_name='Production')        \n",
    "        \n",
    "    dat = extract_date_stl(file, year)\n",
    "    \n",
    "    df['Date'] = dat \n",
    "    df['Month'] = dat.strftime('%B')\n",
    "    df['Weekday'] = dat.strftime('%A')\n",
    "    df['WeekNumber'] = dat.strftime('%U')\n",
    "    df['DOTM'] = dat.strftime('%d')\n",
    "    df['Warehouse'] = 'STL'\n",
    "    \n",
    "    keep_cols = ['Date','Warehouse','LOC','RTE','Driver','Truck#','Stops',\n",
    "                 'TTL Cs/splt','Cs','Btls','Start Hr',\n",
    "                 'End Hr','Ttl Hrs','Ttl Mi','Month','Weekday','WeekNumber',\n",
    "                 'DOTM']\n",
    "    df = df[keep_cols].drop_duplicates()\n",
    "    \n",
    "    WAREHOUSE, ROUTE = df.Warehouse.astype(str), df.RTE.astype(str)\n",
    "    new_index = WAREHOUSE + '_' + ROUTE \n",
    "    \n",
    "    df.set_index(new_index, inplace=True)\n",
    "    \n",
    "    df = df[df['Driver'] != 'Totals:']        \n",
    "    df = df.sort_values(['Stops','TTL Cs/splt'], ascending=False).reset_index(drop=False)\n",
    "    \n",
    "    df['Date'] = df['Date'].replace(to_replace='NaN', value='')\n",
    "    df = df[df['Date'].isnull() == False]\n",
    "    \n",
    "    drop_dumb_shit = lambda col: str(col).lower().replace(' ', '_').replace('#', '').replace('.', '')\n",
    "    df.columns = [drop_dumb_shit(col) for col in df.columns]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Update years like on velocity\n",
    "files_2018 = 'N:\\\\Daily Report\\\\2018\\\\*\\\\*.xls*'\n",
    "files_2017 = 'N:\\\\Daily Report\\\\2017\\\\*\\\\*.xls*'\n",
    "files_2016 = 'N:\\\\Daily Report\\\\2016\\\\*\\\\*.xls*'\n",
    "\n",
    "file_list = {'2016': files_2016, '2017': files_2017, '2018': files_2018}\n",
    "\n",
    "stl_production = pd.DataFrame()        \n",
    "for k, v in file_list.items():\n",
    "    flist = glob.glob(v)\n",
    "    yr = k\n",
    "    for file in flist:\n",
    "        if 'copy' in str(file).lower():\n",
    "            print('Excluding file:  {}'.format(file))\n",
    "            pass\n",
    "        elif '~$' in str(file):\n",
    "            print('Excluding file:  {}'.format(file))\n",
    "            pass\n",
    "        else:\n",
    "            df  = extract_stl_production_tab(file, year=yr)\n",
    "            stl_production = stl_production.append(df)\n",
    "            \n",
    "stl_production.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_cols = ['date', 'warehouse', 'rte']\n",
    "stl_production.sort_values(ix_cols, inplace=True)\n",
    "stl_production.set_index(ix_cols, inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset Date Contained in File Name to Next Day \n",
    "\n",
    "The Daily Report is named for the day of routing, not delivery.  To get the correct delivery day the routing date is changed to the delivery date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_production['date'] = stl_production['date'] + pd.to_timedelta(1, unit='d')\n",
    "stl_production['weekday'] = stl_production['date'].apply(lambda d: d.weekday())\n",
    "wday_map = dict(zip(np.arange(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']))\n",
    "stl_production['weekday'] = stl_production['weekday'].map(wday_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Routes in Daily Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stl_production.loc[stl_production['loc']=='CAPE', 'index'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stl_production.loc[stl_production['loc']=='STL', 'index'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(stl_production.loc[stl_production['loc']=='COL', 'index'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Filter Out Non-Routes***\n",
    "\n",
    "This list was given by Bob Kloeppinger to filter out of the dataset.  Columbia is assumed to send all routes to market.  Cape is being excluded due to its size.\n",
    "\n",
    "Also lines that have no stops are being dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['STL_70', 'STL_91', 'STL_PR', 'STL_nan', 'STL_90', 'STL_93', 'STL_13', 'STL_3', 'STL_25']\n",
    "nontrue_routes = ['STL_70', 'STL_91', 'STL_PR', 'STL_nan', 'STL_90', 'STL_93',     # not true routes\n",
    "                 'STL_13', 'STL_3', 'STL_25', 'STL_nan',                           # not true routes\n",
    "                 'STL_93', 'STL_97', 'STL_98', 'STL_99',                           # from paul C \n",
    "                 'STL_389', 'STL_319',                                    # from paul C combine/sales takes col \n",
    "                 'STL_406', 'STL_407', 'STL_408', 'STL_409', 'STL_410', 'STL_411', # excluding all CAPE\n",
    "                 'STL_412', 'STL_413', 'STL_414', 'STL_415', 'STL_416', 'STL_417', # excluding all CAPE\n",
    "                 'STL_418', 'STL_419', 'STL_420', 'STL_421', 'STL_422', 'STL_423'] # excluding all CAPE\n",
    "nontrue_rtes = stl_production['index'].isin(nontrue_routes)\n",
    "stl_production = stl_production.loc[~nontrue_rtes]\n",
    "# stl_production = stl_production.loc[stl_production['stops'].astype(np.int64) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sams = ['STL_323', 'STL_324'] #sams both col/jeffcity these get combined if rte is below 200 cases\n",
    "print('This Sams issue is not dealt with.  Sometimes they combine these routes, sometimes not.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def stl_daily_route_summary(stl_production):\n",
    "    grpby_df = stl_production.loc[stl_production['loc'].isin(['STL', 'COL'])]\n",
    "    print('Replacing Driver with Route ID for COLUMBIA ONLY because they do not have their Driver Names in the data.')\n",
    "    grpby_df.loc[grpby_df['loc']=='COL', 'driver'] = grpby_df.loc[grpby_df['loc']=='COL', 'rte']\n",
    "    grpby_df['driver'] = grpby_df['driver'].apply(lambda s: 'dr_' + str(s).strip().lower().replace('/', '_').replace(' ', ''))\n",
    "    grpby_df['driver'] = grpby_df['driver'].astype(str)\n",
    "    grp_cols = ['warehouse', 'loc', 'date']\n",
    "    agg_funcs = {'driver': pd.Series.nunique, 'stops': np.sum, 'ttl_cs/splt': np.sum}\n",
    "    print('Counting DRIVER as the route per PAUL C recommendation')\n",
    "    grpby_df = pd.DataFrame(grpby_df.groupby(grp_cols).agg(agg_funcs)).reset_index(drop=False)\n",
    "    grpby_df['date'] = pd.to_datetime(grpby_df['date'])\n",
    "    grpby_df.rename(columns={'driver': 'rte'}, inplace=True)\n",
    "    return grpby_df\n",
    "\n",
    "stl_daily = stl_daily_route_summary(stl_production)\n",
    "stl_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in Calendar Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in calendar data\n",
    "def generate_calendar(year, drop_index=False):\n",
    "    '''\n",
    "    Simple function to generate a calendar containing\n",
    "    US holidays, weekdays and  holiday weeks.\n",
    "    '''\n",
    "    from pandas.tseries.offsets import YearEnd\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    \n",
    "    start_date = pd.to_datetime('1/1/'+str(year))\n",
    "    end_date = start_date + YearEnd()\n",
    "    DAT = pd.date_range(str(start_date), str(end_date), freq='D')\n",
    "    MO = [d.strftime('%B') for d in DAT]\n",
    "    holidays = USFederalHolidayCalendar().holidays(start=start_date, end=end_date)\n",
    "\n",
    "    cal_df = pd.DataFrame({'date':DAT, 'month':MO})\n",
    "    cal_df['year'] = [format(d, '%Y') for d in DAT]\n",
    "    cal_df['weekday'] = [format(d, '%A') for d in DAT]\n",
    "    cal_df['is_weekday'] = cal_df.weekday.isin(['Monday','Tuesday','Wednesday','Thursday','Friday'])\n",
    "    cal_df['is_weekday'] = cal_df['is_weekday'].astype(int)\n",
    "    cal_df['is_holiday'] = cal_df['date'].isin(holidays)\n",
    "    cal_df['is_holiday'] = cal_df['is_holiday'].astype(int)\n",
    "    cal_df['is_holiday_week'] = cal_df.is_holiday.rolling(window=7,center=True,min_periods=1).sum()\n",
    "    cal_df['is_holiday_week'] = cal_df['is_holiday_week'].astype(int)\n",
    "    production_days = ['Tuesday','Wednesday','Thursday','Friday']\n",
    "    cal_df['is_production_day'] = cal_df.weekday.isin(production_days)\n",
    "    cal_df['is_production_day'] = cal_df['is_production_day'].astype(int)\n",
    "    last_biz_day = [str(format(dat, '%Y-%m-%d')) \n",
    "                    for dat in pd.date_range(start_date, end_date, freq='BM')]\n",
    "    cal_df['last_selling_day'] = [int(dat in last_biz_day) for dat in cal_df['date'].astype(str)]\n",
    "    \n",
    "    if not drop_index: cal_df.set_index('date', inplace=True)\n",
    "    \n",
    "    return cal_df\n",
    "\n",
    "def make_calendars(year_list, drop_index):\n",
    "    cal_df = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        cal_df = cal_df.append(generate_calendar(year, drop_index=drop_index))\n",
    "    return cal_df\n",
    "\n",
    "year_list = ['2016', '2017', '2018']\n",
    "cal_df = make_calendars(year_list, drop_index=True)\n",
    "\n",
    "# merge in calendar\n",
    "stl_daily = stl_daily.merge(cal_df, on='date')\n",
    "stl_daily.sort_values(['date', 'loc'], inplace=True)\n",
    "\n",
    "# save to disk\n",
    "stl_daily.to_excel('stl_daily_report__production_tab__jan2016_apr2018.xlsx')\n",
    "\n",
    "stl_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()  # every month\n",
    "yearsFmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "\n",
    "def plot_tseries_over_group_with_histograms(df, xcol, ycol, grpcol, title_prepend='{}', \n",
    "                                            labs=None, x_angle=0, labelpad=60, window=20, \n",
    "                                            vertical_col=None, vertical_label='', logy=False,\n",
    "                                            savefig=False, fname='fig.png'):\n",
    "    '''\n",
    "    Function for plotting time series df[ycol] over datetime range df[xcol]\n",
    "    using the unique_grp_vals contained in df[grpcol].unique().  \n",
    "    \n",
    "     - df: pd.DataFrame containing datetime and series to plot\n",
    "     - xcol: str of column name in df for datetime series\n",
    "     - ycol: str of column name in df for tseries \n",
    "     - grpcol: str of column name in df of group over which to plot\n",
    "     - labs: dict of xlab, ylab\n",
    "     - title_prepend: str containing \"{}\" that prepends group names in title\n",
    "    '''\n",
    "    unique_grp_vals = df[grpcol].unique()\n",
    "    nrows = len(unique_grp_vals) // 2\n",
    "    figsize = (15, 14 * nrows)\n",
    "    fig, axes = plt.subplots(len(unique_grp_vals), 1, figsize=figsize)\n",
    "    title_prepend_hist = 'Histogram of ' + str(title_prepend)\n",
    "    j = 0\n",
    "    for i, grp in enumerate(unique_grp_vals):\n",
    "        _df = df.loc[df[grpcol] == grp]\n",
    "        ax = axes[i]\n",
    "        ax.plot(_df[xcol], _df[ycol], alpha=.7, color='black', linewidth=0.35)\n",
    "        ax.plot(_df[xcol], _df[ycol].rolling(window=window, min_periods=min(5, window)).mean(), \n",
    "                alpha=.8, color='r', label='{} period rolling avg'.format(window),\n",
    "                linestyle='--', linewidth=1.5)\n",
    "        longer_window = int(window * 3)\n",
    "        ax.plot(_df[xcol], _df[ycol].rolling(window=longer_window, min_periods=5).mean(), \n",
    "                alpha=.9, color='darkred', label='{} period rolling avg'.format(longer_window),\n",
    "                linewidth=2.25)\n",
    "        mu, sigma = _df[ycol].mean(), _df[ycol].std()\n",
    "        ax.axhline(mu, linestyle='--', color='r', alpha=.5, linewidth=.3)\n",
    "        ax.axhline(mu - sigma, linestyle='-.', color='y', alpha=.5, linewidth=.3)\n",
    "        ax.axhline(mu + sigma, linestyle='-.', color='y', alpha=.5, linewidth=.3)\n",
    "        ax.set_title(title_prepend.format(grp))\n",
    "        bottom, top = mu - 3*sigma, mu + 3*sigma\n",
    "        ax.set_ylim((bottom, top))\n",
    "        if labs is not None:\n",
    "            ax.set_xlabel(labs['xlab'])\n",
    "            ax.set_ylabel(labs['ylab'])\n",
    "        ax.xaxis.labelpad = labelpad\n",
    "        ax.xaxis.set_minor_locator(months)\n",
    "        ax.grid(alpha=.1)\n",
    "        if x_angle != 0:\n",
    "            for tick in ax.get_xticklabels():\n",
    "                tick.set_rotation(x_angle)\n",
    "        if vertical_col is not None:\n",
    "            for dat, plotit in zip(_df[xcol], _df[vertical_col]):\n",
    "                if plotit == 1: \n",
    "                    ax.axvline(dat, color='lightblue', alpha=.4, label=vertical_label)\n",
    "                    vertical_label = ''\n",
    "        if logy:\n",
    "            ax.set_yscale('log')\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "        divider = make_axes_locatable(ax)\n",
    "        axHisty = divider.append_axes('right', 1.2, pad=0.1, sharey=ax)\n",
    "        axHisty.grid(alpha=.1)\n",
    "        axHisty.hist(_df[ycol].dropna(), orientation='horizontal', alpha=.5, \n",
    "                     color='lightgreen', bins=25)\n",
    "        axHisty.axhline(mu, linestyle='--', color='r', label='mu', \n",
    "                        alpha=.5, linewidth=.3)\n",
    "        axHisty.axhline(mu - sigma, linestyle='-.', color='y', label='+/- two sigma', \n",
    "                       alpha=.5, linewidth=.3)\n",
    "        axHisty.axhline(mu + sigma, linestyle='-.', color='y', alpha=.5, linewidth=.3)\n",
    "        axHisty.legend(loc='best')\n",
    "        \n",
    "        j += 1\n",
    "                \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.despine()\n",
    "    if savefig: plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Only Production Days & Plot Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = 'Number of Trucks to Market for {}'\n",
    "xcol = 'date'\n",
    "ycol = 'rte'\n",
    "grpcol = 'loc'\n",
    "labs = dict(xlab='', ylab='Number of Trucks')\n",
    "vertical_col = 'last_selling_day'#'is_holiday_week'\n",
    "vertical_label = 'last_selling_day'#'is_holiday_week'\n",
    "\n",
    "is_prod_day = stl_daily.is_production_day == True\n",
    "stl_daily_prdday = stl_daily.loc[is_prod_day]\n",
    "\n",
    "plot_tseries_over_group_with_histograms(stl_daily_prdday, xcol, ycol, grpcol,\n",
    "                                       title_prepend, labs, \n",
    "                                       x_angle=90,\n",
    "                                       vertical_col=vertical_col, \n",
    "                                       vertical_label=vertical_label,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_dailyrpt_trucks.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = 'Total Split Cases Delivered for {}'\n",
    "xcol = 'date'\n",
    "ycol = 'ttl_cs/splt'\n",
    "grpcol = 'loc'\n",
    "labs = dict(xlab='', ylab='Split Cases')\n",
    "    \n",
    "plot_tseries_over_group_with_histograms(stl_daily_prdday, xcol, ycol, grpcol, title_prepend, labs,  \n",
    "                                       x_angle=90,\n",
    "                                       vertical_col=vertical_col, \n",
    "                                       vertical_label=vertical_label,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_dailyrpt_splits.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = 'Total Stops for {}'\n",
    "xcol = 'date'\n",
    "ycol = 'stops'\n",
    "grpcol = 'loc'\n",
    "labs = dict(xlab='', ylab='Number of Stops per Day')\n",
    "\n",
    "plot_tseries_over_group_with_histograms(stl_daily_prdday, xcol, ycol, grpcol, title_prepend, labs,  \n",
    "                                       x_angle=90,\n",
    "                                       vertical_col=vertical_col, \n",
    "                                       vertical_label=vertical_label,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_dailyrpt_stops.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Machine Learning\n",
    "\n",
    "We need a model that predicts split cases and a model that predicts number of routes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to better semantically oriented names\n",
    "new_colnames = {'rte': 'routes_per_day', \n",
    "               'ttl_cs/splt': 'split_cases_delivered_per_day', \n",
    "               'stops': 'stops_per_day',\n",
    "               'loc': 'location'}\n",
    "stl_daily_prdday.rename(columns=new_colnames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_train_test(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['bill_and_hold'] = [int(i) for i in df['date'].dt.day.astype(int) <= 5]\n",
    "    df['schlafly'] = [int(i) for i in df['date'] <= pd.to_datetime('2017-07-31')]\n",
    "    \n",
    "    keep_cols = ['location', 'routes_per_day', 'month', 'date',\n",
    "                 'split_cases_delivered_per_day', 'stops_per_day', 'weekday', 'year', \n",
    "                 'last_selling_day', 'is_holiday_week', 'bill_and_hold', 'schlafly']\n",
    "    df = df.loc[:, keep_cols]\n",
    "\n",
    "    dummy_cols = ['location', 'weekday', 'year', 'month']\n",
    "    df = pd.get_dummies(df, columns=dummy_cols, drop_first=False)\n",
    "    drop_space_lowercase = lambda s: str(s).replace(' ', '_').lower()\n",
    "    df.columns = [drop_space_lowercase(col) for col in df.columns]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_modeling = preprocess_train_test(stl_daily_prdday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data to Training & Testing & Scale the Data with `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def split_scale_data(df_modeling, y_col, leave_out, train_size=.8, continuous_cols=[]):\n",
    "    print('''Using X columns:'''); print(X_cols)\n",
    "    print('''To predict Y column:'''); print(y_col)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_modeling[X_cols], \n",
    "                                                        df_modeling[y_col], \n",
    "                                                        train_size=train_size)\n",
    "    if len(continuous_cols) > 0:\n",
    "        std = StandardScaler()\n",
    "        std.fit(X_train[continuous_cols].as_matrix())\n",
    "        X_train[continuous_cols] = std.transform(X_train[continuous_cols])\n",
    "        X_test[continuous_cols] = std.transform(X_test[continuous_cols])\n",
    "    else: \n",
    "        std = None\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'routes_per_day'\n",
    "leave_out = ['date', 'stops_per_day'] + [y_col] \n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "\n",
    "continuous_cols = ['split_cases_delivered_per_day']\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.8,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "print('Shape X_val: {}, Shape X_test: {}'.format(X_val.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "### Test `RandomForestRegressor` Predicting `routes_per_day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(78, 91, 2),\n",
    "             'max_features': np.arange(.5, .71, .02)}\n",
    "model = RandomForestRegressor(n_jobs=-1, \n",
    "                              random_state=777,\n",
    "                              criterion='mse')\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "val_score = r2_score(y_val, [int(pred) for pred in grid.predict(X_val)])\n",
    "test_score = r2_score(y_test, [int(pred) for pred in grid.predict(X_test)])\n",
    "train_score = r2_score(y_train, [int(pred) for pred in grid.predict(X_train)])\n",
    "\n",
    "print('Train Score = {}, Validation Score = {}, Test Score = {}'\n",
    "      .format(train_score, val_score, test_score))\n",
    "\n",
    "joblib.dump(grid.best_estimator_, 'rtes_per_day_model_rf.pkl')\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `RandomForestRegressor` to Predict `split_cases_delivered_per_day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_col = 'split_cases_delivered_per_day'\n",
    "leave_out = ['date', 'stops_per_day', 'routes_per_day'] + [y_col]\n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "continuous_cols = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.8,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(250, 751, 50),\n",
    "             'max_features': np.arange(.30, .45, .02)}\n",
    "model = RandomForestRegressor(n_jobs=-1, \n",
    "                              random_state=777,\n",
    "                              criterion='mse')\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "val_score = r2_score(y_val, [int(pred) for pred in grid.predict(X_val)])\n",
    "test_score = r2_score(y_test, [int(pred) for pred in grid.predict(X_test)])\n",
    "train_score = r2_score(y_train, [int(pred) for pred in grid.predict(X_train)])\n",
    "\n",
    "print('Train Score = {}, Validation Score = {}, Test Score = {}'\n",
    "      .format(train_score, val_score, test_score))\n",
    "\n",
    "joblib.dump(grid.best_estimator_, 'splits_per_day_rf.pkl')\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hogan Data\n",
    "\n",
    "Data below is sent directly from Hogan.\n",
    "\n",
    "Reindex data so zeros show up for all weekdays not used.  Select for only T-F only in all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'N:\\\\Operations Intelligence\\\\Operations Research\\\\Delivery Analysis Post Schlafly\\\\'\n",
    "hogan_xlsx = pd.ExcelFile(base_dir + 'FROM HOGAN Rental_Contract_Revenue_Detail.xlsx')\n",
    "hogan = hogan_xlsx.parse('Rental Contract Revenue Detail', header=0)\n",
    "hogan['Contract Billing Date'] = hogan['Contract Billing Date'].apply(pd.to_datetime)\n",
    "hogan_xlsx.close()\n",
    "hogan.columns = [str(col).replace(' ', '_').lower() for col in hogan.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map in Customer Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_id_map = {'001-30000404-000': 'STL', \n",
    "               '001-30000406-000': 'SPFD', \n",
    "               '001-30000848-000': 'KC', \n",
    "               '001-30000884-000': 'COL', \n",
    "               '001-30007850-000': 'POS'}\n",
    "hogan['loc'] = hogan.customer_number.map(cust_id_map) \n",
    "print('Dropping Van Rentals from Hogan Data')\n",
    "hogan = hogan.loc[hogan['loc'] != 'POS']\n",
    "hogan['loc'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by Day and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Doing Count-Unique Operation on unit_number NOT invoice_number')\n",
    "hogan['invoice_number'] = hogan['unit_number']\n",
    "aggfuncs = {'contract__billed_amount': np.sum, 'invoice_number': pd.Series.nunique}\n",
    "grpcols = ['loc', 'contract_billing_date']\n",
    "hogan_byday = pd.DataFrame(hogan.groupby(grpcols).agg(aggfuncs)).reset_index(drop=False)\n",
    "hogan_byday.rename(columns={'contract_billing_date': 'date', 'loc': 'loc'}, inplace=True)\n",
    "hogan_byday.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spread Negatives Over Entire Series\n",
    "\n",
    "There are quite a few negative days in the data.  To take care of this without losing integrity in the data an iterative approach was taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spread_negatives_evenly(df, col):\n",
    "    below_zero = df[col] < 0\n",
    "    while np.sum(below_zero) >= 1:\n",
    "        negs = df.loc[below_zero, col].abs().sum()\n",
    "        nrow = df.loc[~below_zero, col].shape[0]\n",
    "        spread = np.divide(negs, nrow)\n",
    "        df.loc[~below_zero, col] = np.subtract(df.loc[~below_zero, col], spread) \n",
    "        df.loc[below_zero, col] = 0\n",
    "        below_zero = df[col] < 0\n",
    "    return df\n",
    "\n",
    "hogan_byday = spread_negatives_evenly(hogan_byday, 'contract__billed_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex So Missing Dates Represented in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_fwd_bwd = lambda df_col: df_col.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "hogan_daily = pd.DataFrame()\n",
    "dates = pd.date_range('2016-01-01', '2018-04-25', freq='D')\n",
    "for grp, df in hogan_byday.groupby('loc'):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True, drop=False)\n",
    "    df = df.reindex(dates)\n",
    "    df['loc'] = fill_fwd_bwd(df['loc'])\n",
    "    df = df.fillna(0)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['date'] = df['index']\n",
    "    df.drop(columns='index', inplace=True)\n",
    "    hogan_daily = hogan_daily.append(df)\n",
    "    \n",
    "import math\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, loc in enumerate(hogan_daily['loc'].unique()):\n",
    "    if i < 2: ax = axes[0, i]\n",
    "    else: ax = axes[1, i-2]\n",
    "    n_invoices = hogan_daily.loc[hogan_daily['loc'] == loc, 'invoice_number']\n",
    "    ax.hist(n_invoices, \n",
    "            bins=int(n_invoices.max()), \n",
    "            label='Number of Invoices',\n",
    "            alpha=.7, color=colors[i])\n",
    "    ax.set_xlabel('Bins of Number of Invoices')\n",
    "    ax.set_ylabel('Observations in Bins')\n",
    "    ax.set_title(str(loc))\n",
    "    ax.grid(alpha=.2)\n",
    "    xint = range(0, math.ceil(max(n_invoices)+1))\n",
    "    ax.set_xticks(xint)\n",
    "    sns.despine()\n",
    "sns.set_style('whitegrid')\n",
    "plt.savefig('./figs/histogram_hogan_invoices_per_day.png')\n",
    "plt.show()   \n",
    "\n",
    "agg_funcs = {'contract__billed_amount': {'mean': np.mean, 'std': np.std},\n",
    "            'invoice_number': {'mean': np.mean, 'std': np.std}}\n",
    "hogan_daily.groupby('loc').agg(agg_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Daily Report and Hogan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Hogan Data into Daily Report Data\n",
    "\n",
    "Merging in `stl_daily_prdday` which is the Daily Report's Production Tab from 2016-2018.  This particular dataset contains only production days to avoid some of the noise in off-production-day planned delivery days.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_daily_prdday.columns = [str(col).replace(' ', '_').lower() for col in stl_daily_prdday.columns]\n",
    "stl_daily_prdday.rename(columns={'location': 'loc'}, inplace=True)\n",
    "stl_daily_prdday['loc'] = stl_daily_prdday['loc'].astype(str)\n",
    "stl_daily_prdday.set_index(['date', 'loc'], inplace=True)\n",
    "hogan_daily['loc'] = hogan_daily['loc'].astype(str)\n",
    "hogan_daily.set_index(['date', 'loc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hogan_and_daily_rpt = hogan_daily.join(stl_daily_prdday, how='inner')\n",
    "hogan_and_daily_rpt['amt_per_hogan_invoice'] = np.divide(hogan_and_daily_rpt.contract__billed_amount,\n",
    "                                                        hogan_and_daily_rpt.invoice_number)\n",
    "hogan_and_daily_rpt.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Visualization of Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['contract__billed_amount', 'invoice_number', \n",
    "            'routes_per_day', 'stops_per_day', \n",
    "            'split_cases_delivered_per_day'] #'is_holiday_week', 'last_selling_day'\n",
    "sns.pairplot(hogan_and_daily_rpt, vars=num_cols, hue='loc', diag_kind='kde')\n",
    "plt.savefig('./figs/pairplot_dailyrpt_hogandata.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note we only have only COL and STL now (from Hogan) due to this merge.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogan_and_daily_rpt.groupby('loc')[['amt_per_hogan_invoice', 'invoice_number']].agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogan_and_daily_rpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate by Drawing Samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Predictions for `splits_forecasted`\n",
    "\n",
    "First we need to use the baseline model for split cases delivered.  Recall the columns used to predict this data are simply the `cal_df` in this order:\n",
    "\n",
    "```\n",
    "['last_selling_day', 'is_holiday_week', 'bill_and_hold', 'schlafly', 'location_col', 'location_stl', 'weekday_friday', 'weekday_thursday', 'weekday_tuesday', 'weekday_wednesday', 'year_2016', 'year_2017', 'year_2018', 'month_april', 'month_august', 'month_december', 'month_february', 'month_january', 'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', 'month_september']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data to predict from\n",
    "year_list = ['2016', '2017', '2018']\n",
    "locations = ['STL', 'COL']\n",
    "predictor_cols = ['last_selling_day', 'is_holiday_week', 'bill_and_hold', 'schlafly', \n",
    "                 'location_col', 'location_stl', 'weekday_friday', 'weekday_thursday', \n",
    "                 'weekday_tuesday', 'weekday_wednesday', 'year_2016', 'year_2017', 'year_2018', \n",
    "                 'month_april', 'month_august', 'month_december', 'month_february', \n",
    "                 'month_january', 'month_july', 'month_june', 'month_march', 'month_may',\n",
    "                 'month_november', 'month_october', 'month_september']\n",
    "\n",
    "def generate_split_model_data(year_list, locations, predictor_cols):\n",
    "    cal_df = pd.DataFrame()\n",
    "    for year in year_list:\n",
    "        for loc in locations:\n",
    "            _df = generate_calendar(year, drop_index=True)\n",
    "            _df['location'] = loc\n",
    "            _df = _df.loc[_df.is_production_day == True]\n",
    "            cal_df = cal_df.append(_df)\n",
    "    \n",
    "    cal_df['date'] = pd.to_datetime(cal_df['date'])\n",
    "    cal_df['bill_and_hold'] = [int(i) for i in cal_df['date'].dt.day.astype(int) <= 5]\n",
    "    cal_df['schlafly'] = [int(i) for i in cal_df['date'] <= pd.to_datetime('2018-07-31')]\n",
    "    \n",
    "    dummy_cols = ['month', 'year', 'weekday', 'location']\n",
    "    cal_df = pd.get_dummies(cal_df, dummy_cols, drop_first=False)\n",
    "    cal_df.columns = [str(col).lower() for col in cal_df.columns]\n",
    "    dates = cal_df['date']\n",
    "    cal_df = cal_df[predictor_cols]\n",
    "    return cal_df, dates\n",
    "\n",
    "split_model_data, dates = generate_split_model_data(year_list, locations, predictor_cols)\n",
    "\n",
    "model_splits = joblib.load('splits_per_day_rf.pkl')\n",
    "split_model_data['splits_forecasted'] = model_splits.predict(split_model_data)\n",
    "split_model_data['date'] = dates\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 9))\n",
    "for i, loc in enumerate(locations):\n",
    "    if loc == 'STL': is_stl = True\n",
    "    else: is_stl = False\n",
    "    _df = split_model_data.loc[split_model_data.location_stl == is_stl]\n",
    "    ax = axes[i]\n",
    "    is_2016, is_2017 = _df['date'].dt.year == 2016, _df['date'].dt.year == 2017\n",
    "    is_2018 = (is_2016==False) & (is_2017==False)\n",
    "    ax.plot(_df.loc[is_2016, 'date'], _df.loc[is_2016, 'splits_forecasted'], label='2016')\n",
    "    ax.plot(_df.loc[is_2017, 'date'], _df.loc[is_2017, 'splits_forecasted'], label='2017')\n",
    "    ax.plot(_df.loc[is_2018, 'date'], _df.loc[is_2018, 'splits_forecasted'], label='2018')\n",
    "    ax.grid(alpha=.3)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(str(loc) + ' Split Cases Predicted from Calendar Based Model')\n",
    "    sns.despine()\n",
    "plt.savefig('./figs/split_cases_predicted_calendar_based_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Predictions for `split_cases_delivered_per_day` to Predict `routes_per_day`\n",
    "\n",
    "Recall that the following columns were used as predictors of `routes_per_day`:\n",
    "\n",
    "```\n",
    "['split_cases_delivered_per_day', 'last_selling_day', 'is_holiday_week', 'bill_and_hold', 'schlafly', 'location_col', 'location_stl', 'weekday_friday', 'weekday_thursday', 'weekday_tuesday', 'weekday_wednesday', 'year_2016', 'year_2017', 'year_2018', 'month_april', 'month_august', 'month_december', 'month_february', 'month_january', 'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', 'month_september']\n",
    "```\n",
    "\n",
    "We already predicted `routes_per_day`, and have generated the remainder.  We will use this data as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "model_rtes = joblib.load('rtes_per_day_model_rf.pkl')\n",
    "\n",
    "# note splits_forecasted used instead of split_cases_delivered_per_day\n",
    "predict_cols = ['splits_forecasted', 'last_selling_day', 'is_holiday_week', \n",
    "                'bill_and_hold', 'schlafly', 'location_col', 'location_stl', 'weekday_friday', \n",
    "                'weekday_thursday', 'weekday_tuesday', 'weekday_wednesday', 'year_2016', \n",
    "                'year_2017', 'year_2018', 'month_april', 'month_august', 'month_december', \n",
    "                'month_february', 'month_january', 'month_july', 'month_june', 'month_march', \n",
    "                'month_may', 'month_november', 'month_october', 'month_september']\n",
    "\n",
    "predict_data = split_model_data[predict_cols]\n",
    "predict_data['splits_forecasted'] = predict_data['splits_forecasted'] - predict_data['splits_forecasted'].mean()\n",
    "predict_data['splits_forecasted'] = predict_data['splits_forecasted'] / predict_data['splits_forecasted'].std()\n",
    "\n",
    "split_model_data['rtes_forecasted'] = [floor(p) for p in model_rtes.predict(predict_data)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 9))\n",
    "for i, loc in enumerate(locations):\n",
    "    if loc == 'STL': is_stl = True\n",
    "    else: is_stl = False\n",
    "    _df = split_model_data.loc[split_model_data.location_stl == is_stl]\n",
    "    ax = axes[i]\n",
    "    is_2016, is_2017 = _df['date'].dt.year == 2016, _df['date'].dt.year == 2017\n",
    "    is_2018 = (is_2016==False) & (is_2017==False)\n",
    "    ax.plot(_df.loc[is_2016, 'date'], _df.loc[is_2016, 'rtes_forecasted'], label='2016')\n",
    "    ax.plot(_df.loc[is_2017, 'date'], _df.loc[is_2017, 'rtes_forecasted'], label='2017')\n",
    "    ax.plot(_df.loc[is_2018, 'date'], _df.loc[is_2018, 'rtes_forecasted'], label='2018')\n",
    "    ax.grid(alpha=.3)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(str(loc) + ' Number of Routes Predicted from Calendar Model + Predicted Split Cases')\n",
    "    sns.despine()\n",
    "plt.savefig('./figs/num_routes_predicted_calendar_based_model.png')\n",
    "# split_model_data.to_excel('Predicted Values for Daily Split Cases & Number of Routes.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Delivery Cost Function\n",
    "\n",
    "***Note:  Miles as documented in the Daily Report are NOT reliable.  Thus this information is not considered because it can be easily thrown off by one of the many fat finger instances that happen when tracking this kind of data in Excel***\n",
    "\n",
    "### Labor Cost Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_labor_cost(stops, cases, routes, \n",
    "                     pay_per_stop=0.70, pay_per_case=0.2288, min_daily=200):\n",
    "    daily_labor_cost = pay_per_stop * stops + pay_per_case * cases\n",
    "    adj_daily_labor_cost = min_daily * routes\n",
    "    daily_labor_cost = max(daily_labor_cost, adj_daily_labor_cost)\n",
    "    return daily_labor_cost\n",
    "\n",
    "daily_labor_list = list()\n",
    "for i, row in hogan_and_daily_rpt[['stops_per_day', 'split_cases_delivered_per_day', 'routes_per_day']].iterrows():\n",
    "    dl = daily_labor_cost(row['stops_per_day'], row['split_cases_delivered_per_day'], row['routes_per_day'])\n",
    "    daily_labor_list.append(dl)\n",
    "    \n",
    "hogan_and_daily_rpt['actual_daily_labor_cost'] = daily_labor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost of Leased Vehicles\n",
    "\n",
    "Allocating cost of lease on the basis of number of production days in a given year.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def yearly_lease_cost(n_trucks, avg_monthly_lease_cost_per_truck):\n",
    "    yearly_cost_all_leases = avg_monthly_lease_cost_per_truck * n_trucks * 12\n",
    "    return yearly_cost_all_leases\n",
    "\n",
    "def get_production_days(year='2018'):\n",
    "    prod_days = generate_calendar(year)['is_production_day'].sum()\n",
    "    return prod_days\n",
    "\n",
    "def daily_lease_cost(yearly_production_days, yearly_cost_all_leases):\n",
    "    daily_cost_leases = yearly_cost_all_leases / yearly_production_days\n",
    "    return daily_cost_leases\n",
    "\n",
    "def simulate_delivery_costs(hogan_and_daily_rpt, split_model_data,\n",
    "                            leased_trucks_stl, leased_trucks_col, \n",
    "                            prod_days, avg_monthly_lease_cost_per_truck,\n",
    "                            splits_model = joblib.load('splits_per_day_rf.pkl'), \n",
    "                            rtes_model = joblib.load('rtes_per_day_model_rf.pkl')):\n",
    "    \n",
    "    # derive standard deviations of actuals for introducing randomness\n",
    "    std_splits = hogan_and_daily_rpt.groupby('loc')['split_cases_delivered_per_day'].std().to_dict()\n",
    "    \n",
    "    # add loc to prediction data for mapping\n",
    "    hogan = hogan_and_daily_rpt.copy()\n",
    "    split_model_data['loc'] = split_model_data['location_stl'].map({0: 'COL', 1: 'STL'})\n",
    "    \n",
    "    # map in split predictions before adding randomness\n",
    "    dat_loc = split_model_data['date'].astype(str) + '_' + split_model_data['loc'].astype(str)\n",
    "    splits_forecasted = dict(zip(dat_loc, split_model_data['splits_forecasted']))\n",
    "    hogan['splits_forecasted'] = hogan['date'].astype(str) + '_' + hogan['loc'].astype(str)\n",
    "    hogan['splits_forecasted'] = hogan['splits_forecasted'].map(splits_forecasted)\n",
    "    \n",
    "    # add randomness to total cases\n",
    "    sim_df = hogan.copy()\n",
    "    is_stl, is_col = sim_df['loc']=='STL', sim_df['loc']=='COL'\n",
    "    cases_forecasted = sim_df.loc[is_stl, 'splits_forecasted'].values\n",
    "    sim_df.loc[is_stl, 'splits_forecasted'] = [cs + np.random.randn() * std_splits['STL'] * 0.2\n",
    "                                               for cs in cases_forecasted]\n",
    "    cases_forecasted = sim_df.loc[is_col, 'splits_forecasted'].values\n",
    "    sim_df.loc[is_col, 'splits_forecasted'] = [cs + np.random.randn() * std_splits['COL'] * 0.2\n",
    "                                               for cs in cases_forecasted]\n",
    "    \n",
    "    # now re-predict n_rtes per day using new random variation of cases\n",
    "    # note splits_forecasted used instead of split_cases_delivered_per_day\n",
    "    predict_cols = ['splits_forecasted', 'last_selling_day', 'is_holiday_week', \n",
    "                    'bill_and_hold', 'schlafly', 'location_col', 'location_stl', 'weekday_friday', \n",
    "                    'weekday_thursday', 'weekday_tuesday', 'weekday_wednesday', 'year_2016', \n",
    "                    'year_2017', 'year_2018', 'month_april', 'month_august', 'month_december', \n",
    "                    'month_february', 'month_january', 'month_july', 'month_june', 'month_march', \n",
    "                    'month_may', 'month_november', 'month_october', 'month_september']\n",
    "    predict_data = split_model_data[predict_cols]\n",
    "    predict_data['splits_forecasted'] = predict_data['splits_forecasted'] - predict_data['splits_forecasted'].mean()\n",
    "    predict_data['splits_forecasted'] = predict_data['splits_forecasted'] / predict_data['splits_forecasted'].std()\n",
    "    \n",
    "    # get forecast and map into sim_df\n",
    "    split_model_data['rtes_forecasted'] = [int(p) for p in rtes_model.predict(predict_data)]\n",
    "    rtes_forecasted = dict(zip(dat_loc, split_model_data['rtes_forecasted']))\n",
    "    sim_df['rtes_forecasted'] = sim_df['date'].astype(str) + '_' + sim_df['loc'].astype(str)\n",
    "    sim_df['rtes_forecasted'] = sim_df['rtes_forecasted'].map(rtes_forecasted)\n",
    "    \n",
    "    # get daily labor cost\n",
    "    daily_labor_list = list()\n",
    "    for i, row in sim_df[['stops_per_day', 'splits_forecasted', 'rtes_forecasted']].iterrows():\n",
    "        dl = daily_labor_cost(row['stops_per_day'], row['splits_forecasted'], row['rtes_forecasted'])\n",
    "        daily_labor_list.append(dl)\n",
    "    sim_df['daily_labor_cost'] = daily_labor_list\n",
    "    \n",
    "    # get cost of leases \n",
    "    stl_daily_lease_cost, col_daily_lease_cost = dict(), dict()\n",
    "    for year, days in prod_days.items():\n",
    "        stl_yearly = yearly_lease_cost(leased_trucks_stl, \n",
    "                                       avg_monthly_lease_cost_per_truck)\n",
    "        stl_daily = daily_lease_cost(days, stl_yearly)\n",
    "        stl_daily_lease_cost[year] = stl_daily\n",
    "\n",
    "        col_yearly = yearly_lease_cost(leased_trucks_col, \n",
    "                                       avg_monthly_lease_cost_per_truck)\n",
    "        col_daily = daily_lease_cost(days, col_yearly)\n",
    "        col_daily_lease_cost[year] = col_daily\n",
    "\n",
    "    avg_stl_daily_allocated_lease_cost = np.mean([i for i in stl_daily_lease_cost.values()])\n",
    "    avg_col_daily_allocated_lease_cost = np.mean([i for i in col_daily_lease_cost.values()])\n",
    "\n",
    "    sim_df.loc[is_stl, 'daily_allocated_lease_cost'] = avg_stl_daily_allocated_lease_cost\n",
    "    sim_df.loc[is_col, 'daily_allocated_lease_cost'] = avg_col_daily_allocated_lease_cost\n",
    "    \n",
    "    daily_labor_list = list()\n",
    "    labor_cols = ['stops_per_day', 'splits_forecasted', 'rtes_forecasted']\n",
    "    for i, row in sim_df[labor_cols].iterrows():\n",
    "        dl = daily_labor_cost(stops=row['stops_per_day'], \n",
    "                              cases=row['splits_forecasted'], \n",
    "                              routes=row['rtes_forecasted'])\n",
    "        daily_labor_list.append(dl)\n",
    "\n",
    "    sim_df['daily_labor_cost'] = daily_labor_list\n",
    "\n",
    "    # total cost at the daily level\n",
    "    sim_df['total_delivery_cost'] = sim_df[['daily_allocated_lease_cost', 'daily_labor_cost']].sum(axis=1)\n",
    "    \n",
    "    # select columns and set index\n",
    "    cols_to_round = ['daily_allocated_lease_cost', 'splits_forecasted', 'total_delivery_cost']\n",
    "    sim_df[cols_to_round] = sim_df[cols_to_round].apply(lambda x: round(x, 3))\n",
    "    cols_to_keep = ['date', 'loc', 'weekday', 'daily_labor_cost', \n",
    "                   'daily_allocated_lease_cost', 'splits_forecasted', \n",
    "                   'rtes_forecasted', 'total_delivery_cost', \n",
    "                   'split_cases_delivered_per_day', 'stops_per_day', \n",
    "                   'routes_per_day', 'actual_daily_labor_cost',\n",
    "                   'contract__billed_amount']\n",
    "    sim_df = sim_df[cols_to_keep]\n",
    "    sim_df.rename(columns={'split_cases_delivered_per_day': 'Splits Actual', \n",
    "                           'stops_per_day': 'Stops Actual', \n",
    "                           'routes_per_day': 'Routes Actual'}, inplace=True)\n",
    "    sim_df.rename(columns={'loc': 'location'}, inplace=True)\n",
    "    sim_df.columns = [str(col).replace('_', ' ').title() for col in sim_df.columns]\n",
    "    sim_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # derive secondary metrics for rentals & idle trucks\n",
    "    is_stl, is_col = sim_df['Location']=='STL', sim_df['Location']=='COL'\n",
    "    sim_df.loc[is_stl, 'Truck Leases Active'] = leased_trucks_stl\n",
    "    sim_df.loc[is_col, 'Truck Leases Active'] = leased_trucks_col\n",
    "    stl_differential = sim_df.loc[is_stl, 'Rtes Forecasted'] - sim_df.loc[is_stl, 'Truck Leases Active']\n",
    "    sim_df.loc[is_stl, 'Rentals Needed'] = [max(0, diff) for diff in stl_differential]\n",
    "    sim_df.loc[is_stl, 'Trucks Idle'] = [max(0, -1*diff) for diff in stl_differential]\n",
    "    col_differential = sim_df.loc[is_col, 'Rtes Forecasted'] - sim_df.loc[is_col, 'Truck Leases Active']\n",
    "    sim_df.loc[is_col, 'Rentals Needed'] = [min(max(0, diff), 1) for diff in col_differential]\n",
    "    ## print('Maxing Columbia Rentals at 1 Due to Constraints')\n",
    "    sim_df.loc[is_col, 'Trucks Idle'] = [max(0, -1*diff) for diff in col_differential]\n",
    "    \n",
    "    # derive costs of secondary metrics\n",
    "    avg_rental_stl = 215\n",
    "    avg_rental_col = 395\n",
    "    sim_df.loc[is_stl, 'Idle Truck Cost'] = sim_df.loc[is_stl, 'Trucks Idle'] * avg_stl_daily_allocated_lease_cost\n",
    "    sim_df.loc[is_col, 'Idle Truck Cost'] = sim_df.loc[is_col, 'Trucks Idle'] * avg_col_daily_allocated_lease_cost\n",
    "    sim_df.loc[is_stl, 'Rental Cost'] = sim_df.loc[is_stl, 'Rentals Needed'] * avg_rental_stl\n",
    "    sim_df.loc[is_col, 'Rental Cost'] = sim_df.loc[is_col, 'Rentals Needed'] * avg_rental_stl\n",
    "    sim_df['Total Delivery Cost'] = sim_df['Idle Truck Cost'] + sim_df['Rental Cost'] + sim_df['Total Delivery Cost']\n",
    "    sim_df = sim_df.fillna(0)\n",
    "    \n",
    "    # get actuals\n",
    "    # total cost at the daily level\n",
    "    sim_df.rename(columns={'Contract  Billed Amount': 'Hogan Bill Actual'}, inplace=True)\n",
    "    cost_cols = ['Daily Allocated Lease Cost', 'Actual Daily Labor Cost', 'Hogan Bill Actual']\n",
    "    sim_df['Actual Total Delivery Cost'] = sim_df[cost_cols].sum(axis=1)\n",
    "    \n",
    "    return sim_df\n",
    "\n",
    "\n",
    "prod_days = {'2018': get_production_days('2018'), \n",
    "            '2017': get_production_days('2017'),\n",
    "            '2016': get_production_days('2016')}\n",
    "leased_trucks_stl = 28\n",
    "leased_trucks_col = 9\n",
    "avg_monthly_lease_cost_per_truck = 1425.69 # from nichole murphy\n",
    "\n",
    "sim_df = simulate_delivery_costs(hogan_and_daily_rpt, split_model_data,\n",
    "                                leased_trucks_stl, leased_trucks_col, \n",
    "                                prod_days, avg_monthly_lease_cost_per_truck,\n",
    "                                splits_model = joblib.load('splits_per_day_rf.pkl'), \n",
    "                                rtes_model = joblib.load('rtes_per_day_model_rf.pkl'))\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hypothetical_leases(sim_df, leased_trucks_stl=28, leased_trucks_col=9, \n",
    "                            avg_rental_cost=215.57, inflator=1.0):\n",
    "    is_stl, is_col = sim_df['Location']=='STL', sim_df['Location']=='COL'\n",
    "    sim_df.loc[is_stl, 'Hypothetical Truck Leases'] = leased_trucks_stl\n",
    "    sim_df.loc[is_col, 'Hypothetical Truck Leases'] = leased_trucks_col\n",
    "    daily_lease = np.divide(sim_df['Daily Allocated Lease Cost'], sim_df['Rtes Forecasted']).mean()\n",
    "    sim_df.loc[is_stl, 'Hypothetical Amortized Lease Cost'] = sim_df.loc[is_stl, 'Hypothetical Truck Leases'] * daily_lease\n",
    "    \n",
    "    stl_differential = sim_df.loc[is_stl, 'Routes Actual'] - sim_df.loc[is_stl, 'Truck Leases Active']\n",
    "    sim_df.loc[is_stl, 'Actual Trucks Idle'] = [max(0, -1*diff) for diff in stl_differential]\n",
    "    sim_df.loc[is_stl, 'Actual Idle Cost'] = sim_df.loc[is_stl, 'Actual Trucks Idle'] * avg_rental_cost*inflator\n",
    "    \n",
    "    stl_differential = sim_df.loc[is_stl, 'Rtes Forecasted'] - sim_df.loc[is_stl, 'Hypothetical Truck Leases']\n",
    "    sim_df.loc[is_stl, 'Hypothetical Simulated Rentals Needed'] = [max(0, diff) for diff in stl_differential]\n",
    "    sim_df.loc[is_stl, 'Hypothetical Simulated Trucks Idle'] = [max(0, -1*diff) for diff in stl_differential]\n",
    "    stl_differential = sim_df.loc[is_stl, 'Routes Actual'] - sim_df.loc[is_stl, 'Hypothetical Truck Leases']\n",
    "    sim_df.loc[is_stl, 'Hypothetical Actual Rentals Needed'] = [max(0, diff) for diff in stl_differential]\n",
    "    sim_df.loc[is_stl, 'Hypothetical Actual Trucks Idle'] = [max(0, -1*diff) for diff in stl_differential]\n",
    "    \n",
    "    # get stl costs\n",
    "    sim_df.loc[is_stl, 'Hypothetical Simulated Rental Cost'] = sim_df.loc[is_stl, 'Hypothetical Simulated Rentals Needed'] * avg_rental_cost\n",
    "    sim_df.loc[is_stl, 'Hypothetical Simulated Idle Cost'] = sim_df.loc[is_stl, 'Hypothetical Simulated Trucks Idle'] * avg_rental_cost*inflator #daily_lease*inflator\n",
    "    sim_df.loc[is_stl, 'Hypothetical Actual Rental Cost'] = sim_df.loc[is_stl, 'Hypothetical Actual Rentals Needed'] * avg_rental_cost\n",
    "    sim_df.loc[is_stl, 'Hypothetical Actual Idle Cost'] = sim_df.loc[is_stl, 'Hypothetical Actual Trucks Idle'] * avg_rental_cost*inflator # daily_lease*inflator\n",
    "    \n",
    "    sumcols = ['Hypothetical Actual Rental Cost', \n",
    "               'Hypothetical Actual Idle Cost', \n",
    "               'Hypothetical Amortized Lease Cost']\n",
    "    sim_df.loc[is_stl, 'Hypothetical Actual Total Delivery Cost'] = sim_df.loc[is_stl, sumcols].sum(axis=1)\n",
    "    sumcols = ['Hypothetical Simulated Rental Cost', \n",
    "               'Hypothetical Simulated Idle Cost', \n",
    "               'Hypothetical Amortized Lease Cost']\n",
    "    sim_df.loc[is_stl, 'Hypothetical Simulated Total Delivery Cost'] = sim_df.loc[is_stl, sumcols].sum(axis=1)\n",
    "    sim_df.loc[is_stl, 'Actual Total Delivery Cost - Labor'] = sim_df.loc[is_stl, 'Actual Total Delivery Cost'] - sim_df.loc[is_stl, 'Actual Daily Labor Cost']\n",
    "    \n",
    "    # Columbia not done on purpose for now\n",
    "    \n",
    "    sim_df = sim_df.fillna(0)\n",
    "    \n",
    "    return sim_df\n",
    "\n",
    "sim_df = run_hypothetical_leases(sim_df, leased_trucks_stl=27, leased_trucks_col=9)\n",
    "\n",
    "print('''\n",
    "Actual vs. Forecasted Splits R2-Score = %.4f\n",
    "Actual vs. Forecasted Routes R2-Score = %.4f\n",
    "''' % (r2_score(sim_df['Splits Forecasted'], sim_df['Splits Actual']),\n",
    "          r2_score(sim_df['Rtes Forecasted'], sim_df['Routes Actual'])))\n",
    "\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulation and compare to actuals\n",
    "prod_days = {'2018': get_production_days('2018'), \n",
    "            '2017': get_production_days('2017'),\n",
    "            '2016': get_production_days('2016')}\n",
    "avg_monthly_lease_cost_per_truck = 1425.69 # from nichole murphy\n",
    "splits_model = joblib.load('splits_per_day_rf.pkl')\n",
    "rtes_model = joblib.load('rtes_per_day_model_rf.pkl')\n",
    "inflator = 0.0 #either magnifies or diminishes idle cost per truck per day\n",
    "\n",
    "simulations = pd.DataFrame()\n",
    "lease_range = [25, 26, 27, 28, 29, 30, 31, 32, 33] #ntrucks stl\n",
    "for n, leases_stl in enumerate(lease_range):\n",
    "    for i in range(100):\n",
    "        sim_df = simulate_delivery_costs(hogan_and_daily_rpt, \n",
    "                                         split_model_data,\n",
    "                                         leased_trucks_stl, # varying this BELOW not here\n",
    "                                         leased_trucks_col, \n",
    "                                         prod_days, \n",
    "                                         avg_monthly_lease_cost_per_truck,\n",
    "                                         splits_model, \n",
    "                                         rtes_model)\n",
    "        sim_df = run_hypothetical_leases(sim_df, \n",
    "                                         leased_trucks_stl=leases_stl, # varying STL\n",
    "                                         leased_trucks_col=9,          # hold COL constant\n",
    "                                         inflator=inflator)\n",
    "        sim_df = sim_df.loc[sim_df['Location'] == 'STL']\n",
    "        sim_df.reset_index(inplace=True, drop=False)\n",
    "        sim_df['ix'] = i\n",
    "        sim_df.set_index(['ix', 'Location', 'Date'], inplace=True)\n",
    "        simulations = simulations.append(sim_df)\n",
    "\n",
    "\n",
    "#plot\n",
    "grp_cols = ['Location', 'Hypothetical Truck Leases']\n",
    "agg_funcs = {'Hypothetical Actual Total Delivery Cost': np.mean,\n",
    "            'Hypothetical Simulated Total Delivery Cost': np.mean,\n",
    "            'Actual Total Delivery Cost - Labor': np.mean}\n",
    "summary = simulations.groupby(grp_cols).agg(agg_funcs)\n",
    "summary.rename(columns={'level_2': 'variable', 0: 'value'}, inplace=True)\n",
    "summary.reset_index(drop=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(summary['Hypothetical Truck Leases'], summary['Hypothetical Actual Total Delivery Cost'],\n",
    "       label='Historical Cost if @ Truck Leases', color='black', linestyle='--')\n",
    "ax.plot(summary['Hypothetical Truck Leases'], summary['Hypothetical Simulated Total Delivery Cost'],\n",
    "       label='Simulated Cost if @ Truck Leases', color='blue', linestyle='-.')\n",
    "ax.set_xlabel('Number of Leases')\n",
    "ax.set_ylabel('Average Lease Cost + Rental Cost + Idle Cost per Day')\n",
    "ax.set_title('Average Daily Delivery Cost (STL)\\nIdle Cost is %i x Rental Cost' %inflator)\n",
    "ax.grid(alpha=.3)\n",
    "ax.legend(loc='best')\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "sns.despine()\n",
    "plt.savefig('./figs/cost_function_n_leases_inflating_idle_cost_'+str(inflator)+'.png')\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulation and compare to actuals\n",
    "prod_days = {'2018': get_production_days('2018'), \n",
    "            '2017': get_production_days('2017'),\n",
    "            '2016': get_production_days('2016')}\n",
    "avg_monthly_lease_cost_per_truck = 1425.69 # from nichole murphy\n",
    "splits_model = joblib.load('splits_per_day_rf.pkl')\n",
    "rtes_model = joblib.load('rtes_per_day_model_rf.pkl')\n",
    "#inflator = 1.0 #either magnifies or diminishes idle cost per truck per day\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(17, 12))\n",
    "simulations = pd.DataFrame()\n",
    "inflator_range = [0, .5, 1, 2]\n",
    "\n",
    "for i, inflator in enumerate(inflator_range):\n",
    "    lease_range = [25, 26, 27, 28, 29, 30, 31, 32, 33] #n truck leases stl\n",
    "    for n, leases_stl in enumerate(lease_range):\n",
    "        for j in range(25):\n",
    "            sim_df = simulate_delivery_costs(hogan_and_daily_rpt, \n",
    "                                             split_model_data,\n",
    "                                             leased_trucks_stl, # varying this BELOW not here\n",
    "                                             leased_trucks_col, \n",
    "                                             prod_days, \n",
    "                                             avg_monthly_lease_cost_per_truck,\n",
    "                                             splits_model, \n",
    "                                             rtes_model)\n",
    "            sim_df = run_hypothetical_leases(sim_df, \n",
    "                                             leased_trucks_stl=leases_stl, # varying STL\n",
    "                                             leased_trucks_col=9,          # hold COL constant\n",
    "                                             inflator=i)\n",
    "            sim_df = sim_df.loc[sim_df['Location'] == 'STL']\n",
    "            sim_df.reset_index(inplace=True, drop=False)\n",
    "            sim_df['ix'] =str(i) + '_' + str(n) + '_' + str(j)\n",
    "            sim_df.set_index(['ix', 'Location', 'Date'], inplace=True)\n",
    "            simulations = simulations.append(sim_df)\n",
    "\n",
    "\n",
    "        #plot\n",
    "        if i < 2: ax = axes[0, i]\n",
    "        else: ax = axes[1, i-2]\n",
    "            \n",
    "        grp_cols = ['Location', 'Hypothetical Truck Leases']\n",
    "        agg_funcs = {'Hypothetical Actual Total Delivery Cost': np.mean,\n",
    "                    'Hypothetical Simulated Total Delivery Cost': np.mean,\n",
    "                    'Actual Total Delivery Cost - Labor': np.mean}\n",
    "        summary = simulations.groupby(grp_cols).agg(agg_funcs)\n",
    "        summary.rename(columns={'level_2': 'variable', 0: 'value'}, inplace=True)\n",
    "        summary.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        ax.plot(summary['Hypothetical Truck Leases'], summary['Hypothetical Actual Total Delivery Cost'],\n",
    "               label='Historical Cost if @ Truck Leases', color='black', linestyle='--')\n",
    "        ax.plot(summary['Hypothetical Truck Leases'], summary['Hypothetical Simulated Total Delivery Cost'],\n",
    "               label='Simulated Cost if @ Truck Leases', color='blue', linestyle='-.')\n",
    "        ax.set_xlabel('Number of Leases')\n",
    "        ax.set_ylabel('Average Lease Cost + Rental Cost + Idle Cost per Day')\n",
    "        ax.set_title('Average Daily Delivery Cost (STL)\\nIdle Cost is %i x Rental Cost' %inflator)\n",
    "        hypsimcol = 'Hypothetical Simulated Total Delivery Cost'\n",
    "        ax.axhline(summary[hypsimcol].min(), label='Min of Cost Function', color='lightgreen', linestyle=':')\n",
    "        ax.grid(alpha=.3)\n",
    "        ax.legend(loc='best')\n",
    "        plt.ticklabel_format(style='plain', axis='y')\n",
    "        sns.despine()\n",
    "\n",
    "plt.savefig('./figs/cost_function_n_leases_inflating_idle_cost_'+str(inflator)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Test of the Schlafly Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "def sm_linear_regression(Y, X, intercept=True):\n",
    "    if intercept: X = sm.add_constant(X)\n",
    "    lin_model = sm.OLS(Y, X)\n",
    "    lin_model_results = lin_model.fit()\n",
    "    print(lin_model_results.summary())\n",
    "    return lin_model_results\n",
    "\n",
    "def sm_linear_prediction(X_test, ols_results, intercept=True):\n",
    "    if intercept: X_test = sm.add_constant(X_test)\n",
    "    Y_hat = ols_results.predict(X_test)\n",
    "    return Y_hat\n",
    "\n",
    "y_col = 'split_cases_delivered_per_day'\n",
    "leave_out = ['date', 'stops_per_day', 'routes_per_day', 'year_2016', 'year_2017', 'year_2018',\n",
    "             'month_april', 'month_august', 'month_december', 'month_february', 'month_january', \n",
    "             'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', \n",
    "             'month_september', 'is_holiday_week', 'location_col'] + [y_col]\n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "continuous_cols = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.7,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "\n",
    "ols_results = sm_linear_regression(y_train, X_train)\n",
    "y_predict = sm_linear_prediction(X_test, ols_results)\n",
    "r2_test = r2_score(y_test, y_predict)\n",
    "r2_train = r2_score(y_train, sm_linear_prediction(X_train, ols_results))\n",
    "print('\\nR-squared training = %5f\\nR-squared testing = %5f\\n' %(r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression model is used to test whether if we were to leave out the columns:\n",
    "\n",
    "```\n",
    "# the model is blind to these columns\n",
    "['date', 'stops_per_day', 'routes_per_day', 'year_2016', 'year_2017', 'year_2018',\n",
    " 'month_april', 'month_august', 'month_december', 'month_february', 'month_january', \n",
    " 'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', \n",
    " 'month_september', 'last_selling_day', 'is_holiday_week', 'bill_and_hold']\n",
    "```\n",
    "\n",
    "would we be able to predict whether Schlafly was an active supplier at that time?  This is after adjusting for weekday, bill and hold, holiday weeks, and split cases per day.  The key here is that `split_cases_delivered_per_day` should, if losing Schlafly really \"hurt\" us badly, contain enough signal to predict the `schafly` zero or one indicator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "y_col = 'schlafly'\n",
    "leave_out = ['date', 'stops_per_day', 'routes_per_day', 'year_2016', 'year_2017', 'year_2018',\n",
    "             'month_april', 'month_august', 'month_december', 'month_february', 'month_january', \n",
    "             'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', \n",
    "             'month_september', 'last_selling_day', 'is_holiday_week', 'bill_and_hold'] + [y_col]\n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "continuous_cols = ['split_cases_delivered_per_day']\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.7,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "\n",
    "param_grid = {'C': np.arange(1, 11, 1),\n",
    "             'penalty': ['l1']}\n",
    "model = LogisticRegression(n_jobs=-1, random_state=7)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "val_score = accuracy_score(y_val, [int(pred) for pred in grid.predict(X_val)])\n",
    "test_score = accuracy_score(y_test, [int(pred) for pred in grid.predict(X_test)])\n",
    "train_score = accuracy_score(y_train, [int(pred) for pred in grid.predict(X_train)])\n",
    "\n",
    "print('Train Score = {}, Validation Score = {}, Test Score = {}'\n",
    "      .format(train_score, val_score, test_score))\n",
    "\n",
    "joblib.dump(grid.best_estimator_, 'logistic_test_of_schlafly_effect.pkl')\n",
    "\n",
    "print(grid.best_params_)\n",
    "pd.DataFrame({'coefficients': grid.best_estimator_.coef_[0],\n",
    "             'X_col': X_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `sklearn.linear_model.Lasso`  \n",
    "\n",
    "***Test WITH Schlafly as a predictor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "y_col = 'split_cases_delivered_per_day'\n",
    "leave_out = ['date', 'stops_per_day', 'routes_per_day', 'year_2016', 'year_2017', 'year_2018',\n",
    "             'month_april', 'month_august', 'month_december', 'month_february', 'month_january', \n",
    "             'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', \n",
    "             'month_september', 'last_selling_day', 'is_holiday_week', 'bill_and_hold', 'location_col'] + [y_col]\n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "continuous_cols = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.7,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "\n",
    "param_grid = {'alpha': np.arange(0.1, 10.5, .5)}\n",
    "model = Lasso(random_state=7)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "val_score = r2_score(y_val, [int(pred) for pred in grid.predict(X_val)])\n",
    "test_score = r2_score(y_test, [int(pred) for pred in grid.predict(X_test)])\n",
    "train_score = r2_score(y_train, [int(pred) for pred in grid.predict(X_train)])\n",
    "\n",
    "print('Train Score = {}, Validation Score = {}, Test Score = {}'\n",
    "      .format(train_score, val_score, test_score))\n",
    "\n",
    "joblib.dump(grid.best_estimator_, 'lasso_regression_test_of_schlafly_effect.pkl')\n",
    "\n",
    "print(grid.best_params_)\n",
    "pd.DataFrame({'coefficients': grid.best_estimator_.coef_[0],\n",
    "             'X_col': X_cols})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test WITHOUT Schlafly as a predictor***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "y_col = 'split_cases_delivered_per_day'\n",
    "leave_out = ['date', 'stops_per_day', 'routes_per_day', 'year_2016', 'year_2017', 'year_2018',\n",
    "             'month_april', 'month_august', 'month_december', 'month_february', 'month_january', \n",
    "             'month_july', 'month_june', 'month_march', 'month_may', 'month_november', 'month_october', \n",
    "             'month_september', 'last_selling_day', 'is_holiday_week', 'bill_and_hold', 'location_col', \n",
    "             'schlafly'] + [y_col]\n",
    "X_cols = [col for col in df_modeling.columns if col not in leave_out]\n",
    "continuous_cols = []\n",
    "\n",
    "X_train, X_test, y_train, y_test, std_scaler = split_scale_data(df_modeling, \n",
    "                                                               y_col, \n",
    "                                                               leave_out, \n",
    "                                                               train_size=.7,\n",
    "                                                               continuous_cols=continuous_cols)\n",
    "\n",
    "# split off hold out set\n",
    "pct_hold_out = .5\n",
    "nrow_train = int(pct_hold_out * X_test.shape[0])\n",
    "X_val, y_val = X_test.iloc[:nrow_train], y_test.iloc[:nrow_train]\n",
    "X_test, y_test = X_test.iloc[nrow_train:], y_test.iloc[nrow_train:]\n",
    "\n",
    "param_grid = {'alpha': np.arange(0.1, 10.5, .5)}\n",
    "model = Lasso(random_state=7)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "val_score = r2_score(y_val, [int(pred) for pred in grid.predict(X_val)])\n",
    "test_score = r2_score(y_test, [int(pred) for pred in grid.predict(X_test)])\n",
    "train_score = r2_score(y_train, [int(pred) for pred in grid.predict(X_train)])\n",
    "\n",
    "print('Train Score = {}, Validation Score = {}, Test Score = {}'\n",
    "      .format(train_score, val_score, test_score))\n",
    "\n",
    "# joblib.dump(grid.best_estimator_, 'lasso_regression_test_of_schlafly_effect.pkl')\n",
    "\n",
    "print(grid.best_params_)\n",
    "pd.DataFrame({'coefficients': grid.best_estimator_.coef_[0],\n",
    "             'X_col': X_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl_daily_prdday.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_columns_over_groups(df, cols_to_boxplot, unique_groups, \n",
    "                                grpcol_name, treatment_col,\n",
    "                                savefig=False, fname='fig.png'):\n",
    "    plt.clf()\n",
    "    positions = np.arange(1, df[treatment_col].unique().shape[0]+1)\n",
    "    ncols_to_plot, ngroups = len(cols_to_boxplot), len(unique_groups)\n",
    "    fig, axes = plt.subplots(ncols_to_plot, ngroups, figsize=(ngroups*7, ncols_to_plot*5))\n",
    "    for i, col in enumerate(cols_to_boxplot):\n",
    "        for j, grp in enumerate(unique_groups):\n",
    "            in_group = df[grpcol_name] == grp\n",
    "            _df = df.loc[in_group, [treatment_col, grpcol_name, col]]\n",
    "            treated = _df[treatment_col]==0\n",
    "            ax = axes[i, j]\n",
    "            _df.boxplot(col, by=treatment_col, ax=ax)\n",
    "            ax.set_title(str(col).replace('_', ' ').title() + ' for ' + str(grp))\n",
    "            ax.grid(alpha=.3)\n",
    "            ax.set_xlabel(treatment_col)\n",
    "            ax.set_ylabel(col)\n",
    "    sns.despine()\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_boxplot = ['routes_per_day', 'split_cases_delivered_per_day', 'stops_per_day']\n",
    "unique_groups = stl_daily_prdday['loc'].unique()\n",
    "grpcol_name = 'loc'\n",
    "treatment_col = 'schlafly'\n",
    "    \n",
    "boxplot_columns_over_groups(stl_daily_prdday, cols_to_boxplot, unique_groups, grpcol_name, treatment_col,\n",
    "                           savefig=True, fname='./figs/boxplots_schlafly_comparison_stl_col.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_col = 'weekday'\n",
    "\n",
    "boxplot_columns_over_groups(stl_daily_prdday, cols_to_boxplot, unique_groups, grpcol_name, treatment_col,\n",
    "                           savefig=True, fname='./figs/boxplots_weekday_comparison_stl_col.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Time Series of Number of Invoices and Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogan_daily.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = '$ Amount Billed from Hogan for {}'\n",
    "xcol = 'date'\n",
    "ycol = 'contract__billed_amount'\n",
    "grpcol = 'loc'\n",
    "labs = dict(xlab='', ylab='Invoice Amount')\n",
    "\n",
    "plot_tseries_over_group_with_histograms(hogan_daily, xcol, ycol, grpcol, \n",
    "                                       title_prepend, labs, x_angle=90, logy=False,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_amt_billed_hogan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogan_daily.loc[(hogan_daily.invoice_number == 2) & (hogan_daily['loc'] == 'COL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = '# Invoices Billed for {}'\n",
    "xcol = 'date'\n",
    "ycol = 'invoice_number'\n",
    "grpcol = 'loc'\n",
    "labs = dict(xlab='', ylab='Number of Invoices')\n",
    "\n",
    "plot_tseries_over_group_with_histograms(hogan_daily, xcol, ycol, grpcol, \n",
    "                                        title_prepend, labs, x_angle=90,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_num_invoices_hogan.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery Equipment Leasing 2016-2018\n",
    "\n",
    "This data comes directly from the general ledger and is minimally processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equip_leasing_xlsx = pd.ExcelFile(base_dir + 'Delivery Equipment Leasing  2016-2018.xlsx')\n",
    "rentals = equip_leasing_xlsx.parse('All Data', skiprows=4)\n",
    "rentals.Location = rentals.Location.map({1: 'Kansas City', 2: 'Saint Louis', 3: 'Columbia', 4: 'Springfield'})\n",
    "rentals.Date = rentals.Date.apply(pd.to_datetime)\n",
    "equip_leasing_xlsx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggfuncs = {'Amount': np.sum, 'Reference': pd.Series.nunique}\n",
    "grpcols = ['Location', 'Date']\n",
    "rentals_daily = pd.DataFrame(rentals.groupby(grpcols).agg(aggfuncs)).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(rentals_daily, values=['Amount', 'Reference'], columns=['Location'], index='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentals_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title_prepend = '$ Amount from Accounting for {}'\n",
    "xcol = 'Date'\n",
    "ycol = 'Amount'\n",
    "grpcol = 'Location'\n",
    "labs = dict(xlab='', ylab='Rental $ Incurred')\n",
    "    \n",
    "plot_tseries_over_group_with_histograms(rentals_daily, xcol, ycol, grpcol, \n",
    "                                       title_prepend, labs, x_angle=90,\n",
    "                                       window=2,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_accounting_amount_billed_hogan.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_prepend = 'Unique Invoice References from Accounting for {}'\n",
    "labs = dict(xlab='', ylab='Number of Unique Invoice References')\n",
    "plot_tseries_over_group_with_histograms(rentals_daily, xcol, 'Reference', grpcol, title_prepend, labs, x_angle=90, \n",
    "                                       window=2,\n",
    "                                       savefig=True,\n",
    "                                       fname='./figs/tseries_accounting_invoice_refs_hogan.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
